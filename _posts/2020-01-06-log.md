---
layout: post
title: "2020년 01월 06월 개발로그"
---

[PAPER - LEARNING TO SEE IN THE DARK](#paper-learning-see-in-the-dark)

<a name="paper-learning-see-in-the-dark"></a>

## [PAPER] LEARNING TO SEE IN THE DARK

### EXTREME LOW-LIGHT IMAGING WITH A CONVOLUTIONAL NETWORK

1. Dark indoor environment.
2. The illuminance at the camera is <math><semantics><mrow><mrow/><mo stretchy="false">&lt;</mo><mn>0.1</mn></mrow></semantics></math> lux.
3. The Sony α7SII sensor is exposed for 1/30 second.

![camera output with iso 8,000](/assets/images/learning-to-see-in-the-dark/figure-1-1.png)

_camera output with ISO 8,000_

![camera output with iso 409,600](/assets/images/learning-to-see-in-the-dark/figure-1-2.png)

_camera output with ISO 409,000_

![our result from the raw data](/assets/images/learning-to-see-in-the-dark/figure-1-3.png)

_result from the raw data (camera output with ISO 8,000)_

빛의 양의 적은 상태에서 이미지를 생성하는 것은 어려운 일입니다. 짧게 노출된 이미지의 경우 잡음이 생기고, 길게 노출된 사진의 경우 흐릿하게 보여서 종종 비실용적입니다. 이를 개선하기 위해서 디노이징<sup>denosing</sup>, 디블러링<sup>deblurring</sup>, 개선<sup>enhancement</sup> 등의 기술이 제안되었지만, 밤에 비디오 촬영 속도로 이미지를 생성하는 것과 같은 극한 상황에서는 그 효과가 제한적입니다. 이 논문은 컨볼루션 네트워크를 통한 학습 기반의 이미지 개선 파이프라인을 통해 기존의 저조도 이미지 생성 파이프라인을 대체할 수 있음을 보여줍니다. 짧게 노출 시킨 이미지와 길게 노출 시킨 이미지를 대응한 데이터셋과 컨볼루션 네트워크를 통해서 학습 시킨 결과를 바탕으로 저저도 이미지 생성 프로세스를 학습기반의 이미지 생성 프로세스로 대체할 수 있음을 보여줍니다.




<!-- 빛의 양의 적은 상태에서 이미지를 생성하는 것은 도전적인 과제입니다. 짧게 노출된 이미지의 경우 잡음이 생기고, 길게 노출된 사진의 경우 흐릿하여서 종종 비실용적입니다. 이를 개선하기 위해서 디노이징<sup>denosing</sup>, 디블러링<sup>deblurring</sup>, 개선<sup>enhancement</sup> 등의 기술이 제안되었지만, 밤에 비디오 촬영 속도로 이미지를 생성하는 것과 같은 극한 상황에서 그 효과가 제한적입니다. 이것을 위해서 저조도에서 짧게 노출시킨 이미지와 그와 대응하는 길게 노출 시킨 참조 이미지를 모은 데이터셋를 소개하는 논문입니다. 이 데이터셋을 통해서 저조도 이미지 생성 파이프라인을 개발할 수 있을 것 입니다. 그리고 이 데이터셋을 통해서 컨볼루션 네트워크에 기반한 파이프라인을 개발하였습니다. 이것을 통해서 기존의 저저도 이미지 생성 파이프라인을 학습 기반의 파이프라인으로 대체할 수 있을 것입니다. -->

<!--
----

> 개인적인 번역입니다.

### ABSTRACT

Imaging in low light is challenging due to low proton count and low SNR<sup>signal to noise ratio</sup>.
Short exposure images suffer from noise, while long exposure can induce blur and is often impractical. A variety of denosing, deblurring, and enhancement techniques have been proposed, but their effectiveness is limited in extream conditions, such as video-rate imaging at night. To support the development of learning based pipelines for low-light image processing, we introduce a dataset of raw short exposure low light images, with corresponding long exposure reference images. Using the presented dataset, we develop a pipeline for processing low light images, based on end-to-end training of a fully convolutional network. The network operates directly on raw sensor data and replaces much of the traditional image processing pipeline, which tends to perform poorly on such data. We report promising results on the new dataset, analyze factors that affect performance, and highlight opportunities for future work.


양자의 양의 적은 상태에서 그리고 잡음 대비 신호가 낮은 상태에서의 이미지를 생성하는 것은 도전적인 일입니다. 짧은 노출은 잡음을 만들고 반면에 긴 노출은 흐릿함을 만듭니다. 이것을 개선하기 위하여 디노이징, 디블러링, 그리고 강화등의 여러 기술들이 제안되었지만 어두운 밤에 비디오 촬영 속도로 이미지를 생성하는 것과 같은 극한의 환경에서는 그들의 효과가 미미합니다. 저조도에서의 이미지 생성을 위한 학습 기반의 파이프라인을 개발을 지원하기 위해서 저조도에서 짧게 노출한 원본 이미지 원본과 각각 길게 노출된 참조 이미지들을 연관시킨 데이터 셋을 소개합니다. 이 데이터들을 이용해서 end-to-end training of a fully convolutional network 에 기반한 파이프라인을 개발했습니다. 네트워크는 센서 데이터에서 직접 작동하며 기존 이미지 처리 파이프 라인의 대부분을 대체합니다. 기존의 파이프 라인은 데이터에서 성능이 저하되는 경향이 있습니다.

### 1. INTRODUCTION

Noise is present in any imaging system, but it makes imaging particularly challenging in low light. High ISO can be used to increase brightness, but it also amplifies noise. Post-processing, such as scaling or histogram stretching, can be applied, but this does not resolve the low signal-to-noise ratio due to low photon counts. There are physical means to increase signal-to-noise ratio in low light, including opening the aperture, extending exposure time, and using flash. But each of these has its own characteristic drawbacks. For example, increasing exposure time can introduce blur due to camera shake or object motion.

The challenge of fasting imaging in low light is well known in the computational photography community, but remains open. Researchers have proposed techniques for denoising, deblurring, and enhancement of low light images. These techniques generally assume that images are captured in somewhat dim environments with moderate levels of noise. In contrast, we are interested in extreme low light imaging with severely limited illumination (e.g., moonlight) and short exposure (ideally at video rate). In this regime, the traditional camera processing pipeline breaks down and the image has to be reconstructed from the raw sensor data.

Figure 1 illustrates our setting. The environment is extremely dark: less than 0.1 lux of illumination at the camera. The exposure time is set to 1/30 second. The aperture is f/5.6. At ISO 8,000, which is generally considered high, the camera produces an im age that is essentially black, despite the high light sensitivity of the full-frame Sony sensor. At ISO 409,600, which is far beyond the reach of most cameras, the content of the scene is discernible, but the image is dim, noisy, and the colors are distorted. As we will show, even state-of-the-art denoising techniques fail to remove such noise and do not address the color bias. An alternative approach is to use a brust of images, but burst alignment algorithms may fail in extreme low light conditions and burst pipelines are not designed for video capture (e.g., due to the use of 'lucky imaging' within the burst).

We propose a new image processing pipeline that addresses the challenges of extreme low light photography via a data-driven approach. Specifically, we train deep neural networks to learn the image processing pipeline for low light raw data, including color transformations, demosaicing, noise reduction, and image enhancement. The pipeline is trained end-to-end to avoid the noise amplification and error accumulation that characterize traditional camera processing pipelines in this regime.

Most existing methods for processing low light images were evaluated on synthetic data or on real low light images without ground truth. To the best of our knowledge, there is no public dataset for training and testing techniques for processing fast low light images with diverse real world data and ground truth. Therefor, we have collected a new dataset of raw images captured with fast exposure in low light conditions. Each low light image has a corresponding long exposure high-quality reference image. We demonstrate are amplified by up to 300 times with successful nosie reduction and correct color transformation. We systematically analyze key elements of the pipeline and discuss directions for future research.





> [Learning to See in the Dark](https://arxiv.org/abs/1805.01934)
> [ISO 감도](/wiki/camera/iso/)
> [LUX](/wiki/lux/)
- [SIGNAL TO NOISE RATIO](/wiki/signal-to-noise-ratio/)

| amplify | ...을 확대하다. 증폭하다. 부연하다 |
| resolve | 해결하다. 결의하다. 대책을 마련하다. 해법 |
| aperture | 조리개, 구멍, 틈, 렌즈의 구경 |
| drawback | 결점, 단점, 결함 |
| dim | 어두운, 조명을 줄이는, 불투명한, 우둔한 |
| moderate | 사회를 보다. 온건한, 적당한, 중도의, 완화하다. |
| discernible | 인식할 수 있는, 보고 알 수 있는, 알아볼 수 있는 |
| distorted | 잘못 전해진, 왜곡한, 정신적으로 비뚤어진 |
| demosaicing | 디모자익 |
| mosaic | 모자이크 |
| accumulation | 축적, 누적, 축적물 |
| ground truth | 공중 탐사 결과를 검증하기 위하여 지상 조사로 얻은 정보 |
| diverse | 다양한, 여러 가지의, 다른, 광범위한, 각계 각층의 |

-->

----


[OPENCV JAVASCRIPT - CHANGING COLORSPACES](#opencv-js-change-colorspaces)<br />
[OPENCV JS - GEOMETRIC TRANSFORMATIONS OF IMAGES](#opencv-js-geometric-transformations-of-images)

__PERSPECTIVE TRANSFORMATION__

<img id="perspective-transformation-input" src="/assets/images/first.jpg">

<canvas id="perspective-transformation-output"></canvas>

<script>
  function perspectiveTransformRun(){
    let source = cv.imread('perspective-transformation-input');
    let destination = new cv.Mat();
    let dsize = new cv.Size(source.rows, source.cols);
    let src = cv.matFromArray(4, 1, cv.CV_32FC2, [10, 30, 88, 10, 28, 97, 99, 90]);
    let dst = cv.matFromArray(4, 1, cv.CV_32FC2, [0, 0, 50, 0, 0, 50, 50, 50]);

    try {
      let m = cv.getPerspectiveTransform(src, dst);
      cv.warpPerspective(source, destination, m, dsize, cv.INTER_LINEAR, cv.BORDER_CONSTANT, new cv.Scalar());
    } catch(e) {
      console.log(e);
    }

    cv.imshow('perspective-transformation-output', destination);
    source.delete();
    destination.delete();
    m.delete();
    src.delete();
    dst.delete();
  }
  dispatch(perspectiveTransformRun);
</script>

For perspective transformation, you need a 3 x 3 tranformation matrix. Straight lines will remain straight even after the tranformation. To find this transformation matrix, you need 4 points on the input image and corresponding points on the output image. Among these 4 points, 3 of them should not be collinear. Then transformation matrix can be found by the function cv.getPerspectiveTransform. Then apply cv.warpPerspective with this 3x3 transformation matrix.

```
cv.warpPerspective(source, destination, dsize, flags = cv.INTER_LINEAR, borderMode = cv.BORDER_CONSTANT, borderValue = new cv.Scalar())
```

| source      | input image |
| destination | output image that has the size dsize and the same type as source. |
| mat         | 3 x 3 transformation matrix (cv.CV_64FC1 type) |
| dsize       | size of the output image.                      |
| flags       | combination of interpolation methods (cv.INTER_LINEAR or cv.INTER_NEAREST) and the optional flag WARP_INVERSE_MAP, that sets M as the inverse transformation |
| borderMode  | pixel extrapolation method (cv.BORDER_CONSTANT or cv.BORDER_REPLICATE) |
| borderValue | value used in case of a constant border; by default, it is 0 |

cv.getPerspectiveTransform(source, destination)

| source | coordinate of quadrangle vertices in the source images. |
| destination | coordinate of the corresponding quadrange vertices in the destination image. |

__AFFINE TRANSFORMATION__


<img id="affine-transformation-input" src="/assets/images/first.jpg">

<canvas id="affine-transformation-output"></canvas>

<script>
  function runGetAffineTransformation() {
    let source = cv.imread('affine-transformation-input');
    let destination = new cv.Mat();
    let src = cv.matFromArray(3, 1, cv.CV_32FC2, [  0,   0,   0,   1,   1,   0]);
    let dst = cv.matFromArray(3, 1, cv.CV_32FC2, [0.6, 0.2, 0.1, 1.3, 1.5, 0.3]);
    let dsize = new cv.Size(source.rows, destination.cols);
    let m = cv.getAffineTransform(src, dst);
    cv.warpAffine(source, destination, m, dsize, cv.INTER_LINEAR, cv.BORDER_CONSTANT, new cv.Scalar());
    cv.imshow('affine-transformation-output', destination);
    source.delete();
    destination.delete();
    m.delete();
    src.delete();
    dst.delete();
  }
  dispatch(runGetAffineTransformation);
</script>

In affine transformation, all parallel lines in the original image will still be paralle in the output image. To find the transformation matrix, we need three points from input image and their corresponding locations in output image. Then cv.getAffineTransform will create a 2x3 matrix which is to be passed to cv.warpAffine.

```
cv.getAffineTransform(source, destination)
```

| source | three points([3, 1] size and cv.CV_32FC2 type) from input imag.
| destination | three corresponding points [3, 1] size and cv.CV_32FC2 type in output image.

__ROTATION__

<img id="rotation-input" src="/assets/images/first.jpg">

<canvas id="rotation-output"></canvas>

<script>
  function rotationRun() {
    let source = cv.imread('rotation-input');
    let destination = new cv.Mat();
    let dsize = new cv.Size(source.rows, destination.cols);
    let center = new cv.Point(source.cols/2, source.rows/2);
    let m = cv.getRotationMatrix2D(center, 45, 1);
    cv.warpAffine(source, destination, m, dsize, cv.INTER_LINEAR, cv.BORDER_CONSTANT, new cv.Scalar());
    cv.imshow('rotation-output', destination);
    source.delete();
    destination.delete();
    m.delete();
  }
  dispatch(rotationRun);
</script>

----

Rotation of an image for an angle <math><semantics><mi>θ</mi></semantics></math> is achieved by the transformation matrix of the form

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
 <semantics>
  <mrow>
   <mi>M</mi>
   <mo stretchy="false">=</mo>
   <mrow>
    <mo fence="true" stretchy="true">|</mo>
    <mrow>
     <mtable>
      <mtr>
       <mtd>
        <mrow>
         <mi>cos</mi>
         <mi>θ</mi>
        </mrow>
       </mtd>
       <mtd>
        <mrow>
         <mrow>
          <mo stretchy="false">−</mo>
          <mi>sin</mi>
         </mrow>
         <mi>θ</mi>
        </mrow>
       </mtd>
      </mtr>
      <mtr>
       <mtd>
        <mrow>
         <mi>sin</mi>
         <mi>θ</mi>
        </mrow>
       </mtd>
       <mtd>
        <mrow>
         <mi>cos</mi>
         <mi>θ</mi>
        </mrow>
       </mtd>
      </mtr>
     </mtable>
    </mrow>
    <mo fence="true" stretchy="true">|</mo>
   </mrow>
  </mrow>
 </semantics>
</math>

But OpenCV provides scaled rotation with adjustable center of rotation so that you can rotate at any location you prefer. Modified transformation matrix is given by

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
 <semantics>
  <mrow>
   <mi>M</mi>
   <mo stretchy="false">=</mo>
   <mrow>
    <mo fence="true" stretchy="true">|</mo>
    <mrow>
     <mtable>
      <mtr>
       <mtd>
        <mi>α</mi>
       </mtd>
       <mtd>
        <mi>β</mi>
       </mtd>
       <mtd>
        <mrow>
         <mrow>
          <mrow>
           <mo fence="true" stretchy="false">(</mo>
           <mrow>
            <mrow>
             <mn>1</mn>
             <mo stretchy="false">−</mo>
             <mi>α</mi>
            </mrow>
           </mrow>
           <mo fence="true" stretchy="false">)</mo>
          </mrow>
          <mo stretchy="false">⋅</mo>
          <mi mathvariant="italic">center</mi>
         </mrow>
         <mi>.</mi>
         <mrow>
          <mi>x</mi>
          <mo stretchy="false">−</mo>
          <mrow>
           <mi>β</mi>
           <mo stretchy="false">⋅</mo>
           <mi mathvariant="italic">center</mi>
          </mrow>
         </mrow>
         <mi>.</mi>
         <mi>y</mi>
        </mrow>
       </mtd>
      </mtr>
      <mtr>
       <mtd>
        <mrow>
         <mo stretchy="false">−</mo>
         <mi>β</mi>
        </mrow>
       </mtd>
       <mtd>
        <mi>α</mi>
       </mtd>
       <mtd>
        <mrow>
         <mrow>
          <mi>β</mi>
          <mo stretchy="false">⋅</mo>
          <mi mathvariant="italic">center</mi>
         </mrow>
         <mi>.</mi>
         <mrow>
          <mi>x</mi>
          <mo stretchy="false">−</mo>
          <mrow>
           <mrow>
            <mo fence="true" stretchy="false">(</mo>
            <mrow>
             <mrow>
              <mn>1</mn>
              <mo stretchy="false">−</mo>
              <mi>α</mi>
             </mrow>
            </mrow>
            <mo fence="true" stretchy="false">)</mo>
           </mrow>
           <mo stretchy="false">⋅</mo>
           <mi mathvariant="italic">center</mi>
          </mrow>
         </mrow>
         <mi>.</mi>
         <mi>y</mi>
        </mrow>
       </mtd>
      </mtr>
     </mtable>
    </mrow>
    <mo fence="true" stretchy="true">|</mo>
   </mrow>
  </mrow>
  <annotation encoding="StarMath 5.0">M = left lline {
matrix {
  %alpha # %beta # { (1-%alpha) cdot center.x - %beta cdot center.y} ##
  -%beta # %alpha # { %beta cdot center.x - (1-%alpha) cdot center.y}
}
} right rline</annotation>
 </semantics>
</math>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
 <semantics>
  <mrow>
   <mrow>
    <mi>α</mi>
    <mo stretchy="false">=</mo>
    <mrow>
     <mi mathvariant="italic">scale</mi>
     <mo stretchy="false">⋅</mo>
     <mi>cos</mi>
    </mrow>
   </mrow>
   <mi>θ</mi>
  </mrow>
  <annotation encoding="StarMath 5.0">%alpha = scale cdot cos %theta</annotation>
 </semantics>
</math>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
 <semantics>
  <mrow>
   <mrow>
    <mi>β</mi>
    <mo stretchy="false">=</mo>
    <mrow>
     <mi mathvariant="italic">scale</mi>
     <mo stretchy="false">⋅</mo>
     <mi>sin</mi>
    </mrow>
   </mrow>
   <mi>θ</mi>
  </mrow>
  <annotation encoding="StarMath 5.0">%beta = scale cdot sin %theta</annotation>
 </semantics>
</math>

```
cv.getRotatioMatrix2D(center, angle, scale)
```

| center | center of the rotation in the source image |
| angle  | rotation angle in degress. Positive values mean counter-clockwise (the coordinate origin is assumed to be the top-left cornet). |
| scale  | isotropic scale factor. |




<a name="opencv-js-geometric-transformations-of-images"></a>
## [OPENCV JS - GEOMETRIC TRANSFORMATIONS OF IMAGES]

<img width="100" id="transformation-input" src="/assets/images/first.jpg">

<canvas width="100" id="transformation-output"></canvas>

<script type="text/javascript">
  function TransformationRun() {
    let source = cv.imread('transformation-input');
    let destination = new cv.Mat();
    let M = cv.matFromArray(2, 3, cv.CV_64FC1, [1, 0, 10, 0, 1, 20]);
    let dsize = new cv.Size(source.rows, source.cols);
    cv.warpAffine(source, destination, M, dsize, cv.INTER_LINEAR, cv.BORDER_CONSTANT, new cv.Scalar());
    console.log(destination.rows);
    console.log(destination.cols);
    cv.imshow('transformation-output', destination);
    source.delete();
    destination.delete();
    M.delete();
  }
  dispatch(TransformationRun);
</script>

### GOAL

Learn how to apply different geometric transformation to image like translation, rotation, affine transformation etc.

### TRANSFORMATIONS

__TRANSLATION__

Translation is the shifting of object's location. If you know the shift in (x, y) direction, let it be (t<sub>x</sub>, t<sub>y</sub>), you can create the transformation matrix M as follows:

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
 <semantics>
  <mrow>
   <mi>M</mi>
   <mo stretchy="false">=</mo>
   <mrow>
    <mo fence="true" stretchy="true">|</mo>
    <mrow>
     <mtable>
      <mtr>
       <mtd>
        <mn>1</mn>
       </mtd>
       <mtd>
        <mn>0</mn>
       </mtd>
       <mtd>
        <msub>
         <mi>t</mi>
         <mi>x</mi>
        </msub>
       </mtd>
      </mtr>
      <mtr>
       <mtd>
        <mn>0</mn>
       </mtd>
       <mtd>
        <mn>1</mn>
       </mtd>
       <mtd>
        <msub>
         <mi>t</mi>
         <mi>y</mi>
        </msub>
       </mtd>
      </mtr>
     </mtable>
    </mrow>
    <mo fence="true" stretchy="true">|</mo>
   </mrow>
  </mrow>
  <annotation encoding="StarMath 5.0">M = left lline {
matrix {
  1 # 0 # t_x ##
  0 # 1 # t_y
}
} right rline</annotation>
 </semantics>
</math>

```
cv.warpAffine(source, destination, M, dsize, flags = cv.INTER_LINEAR, borderMode = cv.BORDER_CONSTANT, borderValue = new cv.Scalar())
```

| source | input image |
| destination | output image that has the dsize and the same type as source |
| m           | 2 x 3 transfomration matrix |
| dsize       | size of the output image |
| flags       | combination of interploation methods and the optional flag WARP_INVERSE_MAP that means that matrix is the inverse transformation |
| borderMode  | pixel extrapolation method; the borderMode = BORDER_TRANSPARENT, it means that the pixels in the destination image corresponding to the "outliers" in the source image are not modified by the function. |
| borderValue | value used in case of a constant border; byt default, it is 0 |

__SCALING__

Scaling is just resize of the image. OpenCV comes with a function `cv.resize()` for this purpose. The size of the image can be specified manually, or you can specify the scaling factor. Different interpolation methods are used. Preferable interpolation methods are cv.INTER_AREA for shrinking and cv.INTER_CUBIC (slow) & cv.INTER_LINEAR for zooming.

```
cv.resize(source, destination, dsize, fx = 0, fy = 0, interpolation = cv.INTER_LINEAR)
```

| source      | input image |
| destination | output image; it has the size dsize (when it is non-zero) or the size computed from source.size(), fx, and fy; the type of destination is the same as of source. |
| dsize       | output image size; if it equals zero, it is computed as dsize = size(round(fx * source.cols), round(fy * source.rows)) either dsize of both fx and fy must be non-zero |
| fx          | scale factor along the horizontal axis; when it equals 0, it is computed as (double) dsize.width/source/cols |
| fy          | scale factor along the vertical axis; when it equals 0, it is computed as (double)dsize.height/source.rows |
| interplation | interpolation method |

| interpolation | 삽입 |

<img id="opencv-js-resize-input" src="/assets/images/first.jpg">

<canvas width="100" height="100" id="opencv-js-resize-output"></canvas>

<script type="text/javascript">
  function opencvJsResizeRun() {
    let source = cv.imread('opencv-js-resize-input');
    let destination = new cv.Mat();
    let dsize = new cv.Size(100, 100);
    cv.resize(source, destination, dsize, 0, 0, cv.INTER_AREA);
    cv.imshow('opencv-js-resize-output', destination);
    source.delete();
    destination.delete();
  }
  dispatch(opencvJsResizeRun);
</script>

<a name="opencv-js-change-colorspaces"></a>
## [OPENCV JS - CHANGE COLORSPACES]

### GOAL

- In this tutorial, you will learn how to convert images from one color space to another, like RGB -> GRAY, RGB -> HSV etc.
- You will learn following functions: cv.cvtColor(), cv.inRange() etc.

### CONVERT COLOR

There are more than 150 color space conversion methods available in OpenCV. But we will look into the most widely used one: RGB and Gray.

```
cv.cvtColor(source, destination, code, dstCn = 0)
```

| source | input image |
| destination | output image of the same size and depth as source |
| code | color space conversion code |
| dstCn | number of channels in the destination image; if the parameter is 0, the number of the channels is derived automatically from source and code. |

<img id="opencv-js-change-colorspaces-input" src="/assets/images/first.jpg" width="100">

<canvas id="opencv-js-change-colorspaces-output"></canvas>

<script type="text/javascript">
  function opencvChangeColorspacesInputRun() {
    let o = document.getElementById('opencv-js-change-colorspaces-input');
    document.getElementById('opencv-js-change-colorspaces-output').width = o.clientWidth;
    document.getElementById('opencv-js-change-colorspaces-output').height = o.clientHeight;
    let source = cv.imread('opencv-js-change-colorspaces-input');
    let destination = new cv.Mat();
    cv.cvtColor(source, destination, cv.COLOR_RGBA2GRAY, 0);
    cv.imshow('opencv-js-change-colorspaces-output', destination);
    source.delete();
    destination.delete();
  }
  dispatch(opencvChangeColorspacesInputRun);
</script>

### IN RANGE

Check if array elements lie between the elements of two other arrays.

```
cv.inRange(source, lowerb, upperb, dst)
```

| source | first input image |
| lowerb | inclusive lower boundary Mat of the same size as source |
| upperb | inclusive upper boundary Mat of the same size as source |
| dst    | output image of the same size as source and cv.CV_8U type |

<img id="opencv-js-change-colorspaces-inrange-input" src="/assets/images/first.jpg" width="100">

<canvas id="opencv-js-change-colorspaces-inrange-output"></canvas>

<script type="text/javascript">
  function opencvJsChangeColorspacesInRangeRun() {
    let o = document.getElementById('opencv-js-change-colorspaces-inrange-input');
    document.getElementById('opencv-js-change-colorspaces-inrange-output').width = o.clientWidth;
    document.getElementById('opencv-js-change-colorspaces-inrange-output').height = o.clientHeight;
    let source = cv.imread('opencv-js-change-colorspaces-inrange-input');
    let destination = new cv.Mat();
    let low = new cv.Mat(source.rows, source.cols, source.type(), [0, 0, 0, 0]);
    let high = new cv.Mat(source.rows, source.cols, source.type(), [150, 150, 150, 255]);
    cv.inRange(source, low, high, destination);
    cv.imshow('opencv-js-change-colorspaces-inrange-output', destination);
    source.delete();
    destination.delete();
    low.delete();
    high.delete();
  }
  dispatch(opencvJsChangeColorspacesInRangeRun);
</script>

<!--
<a name="mathematics"></a>
## MATHEMATICS

__정의__ <math><mi>a</mi></math>와 같지는 않지만 <math><mi>a</mi></math>에 충분히 가까운 <math><mi>x</mi></math>를 잡으면 <math><mi>L</mi></math>에 얼마든지 가까운 <math><semantics><mrow><mi>f</mi><mrow><mo fence="true" stretchy="false">(</mo><mrow><mi>x</mi></mrow><mo fence="true" stretchy="false">)</mo></mrow></mrow></semantics></math> 값을 얻을 수 있을 때

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
 <semantics>
  <mrow>
   <mrow>
    <munder>
     <mi>lim</mi>
     <mrow>
      <mi>x</mi>
      <mo stretchy="false">→</mo>
      <mi>a</mi>
     </mrow>
    </munder>
    <mrow>
     <mi>f</mi>
     <mrow>
      <mo fence="true" stretchy="false">(</mo>
      <mrow>
       <mi>x</mi>
      </mrow>
      <mo fence="true" stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo stretchy="false">=</mo>
   <mi>L</mi>
  </mrow>
  <annotation encoding="StarMath 5.0">lim from { x -&gt; a } { f(x) } = L</annotation>
 </semantics>
</math>

로 나타내고 "<math><mi>x</mi></math>가 <math><mi>a</mi></math>에 접근할 때 <math><semantics><mrow><mi>f</mi><mrow><mo fence="true" stretchy="false">(</mo><mrow><mi>x</mi></mrow><mo fence="true" stretchy="false">)</mo></mrow></mrow></semantics></math>의 극한은 <math><mi>L</mi></math>이다."라고 말한다.
-->
