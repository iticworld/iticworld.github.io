---
layout: post
title: "2019년 12월 26일"
---

## CALCULUS

### 1 FUNCTIONS AND LIMITS

The fundamental objects that we deal with in calculus are functions. We stress that a function can be represented in different ways: by an equation, in a table, by a graph, or in words. We look at the main types of functions that occur in calculus and describe the process of using these functions as mathematical models of real world phenomena.

In a preview of calculus we saw how the idea of a limit underlies the various branches of calculus. It is therefor appropriate to begin our study of calculus by investigating limits of functions and their properties.

미분적분학에서 다루는 기본 목표는 함수입니다. 우리는 함수가 방정식, 표, 그래프 또는 단어등으로 다른 방식으로 표현될 수 있다고 강조합니다. 미적분학에서 발생하는 주요 유형의 함수를 살펴보고 이러한 함수를 실제 현상의 수학적 모델로 사용하는 프로세스를 설명합니다.

이 책의 미리보기에서 우리는 극한의 개념이 미적분의 다양한 가지의 기초가 되는 방법을 보았습니다. 함수와 그 속성의 극한을 조사하여 미적분학 연구를 시작하는 것이 적절합니다.

#### 1.1 FOUR WAYs TO REPRESENT A FUNCTION

A function f is a rule that assigns to each element x in a set D exactly one element, called f(x), in a set E.

함수는 집합의 각 요소가 연산을 통해서 다른 집합의 정확하게 하나의 요소에 할당되는 규칙입니다.

We usually consider functions for which the sets D and E are sets of real numbers. The set D is called domain of the function. The number f(x) is the value of f at x and is read "f of x". The range of f is the set all possible values of f(x) as x varies throughout the domain. A symbol that represents an arbitrary number in the domain of a function f is called an independent variable. A symbol that represents a number in the range of f is called a depedent variable. In Example A, for instance, r is the independent variable and A is the dependent variable.

우리는 일반적으로 집합 D 와 E가 실수 집합인 함수를 고려합니다. 집합 D를 함수의 도메인이라고 합니다. 숫자 f(x) 는 x 에서의 f의 값이며, "f of x" 로 읽습니다. f 의 범위는 x 가 도메인 전체에 따라 달라짐에 따라 f(x)의 가능한 모든 값을 설정합니다. 함수 f의 도메인에서 임의의 숫자를 나타내는 기호를 독립변수라고 합니다. f 범위의 숫자를 나타내는 기호를 종속변수라고 합니다. 예를 들어, f(x) = 3x 에서 x 는 독립변수이고 f(x) 는 종속변수입니다.

It's helpful to think of a function as a machine. If x is in the domain of th function f, then when x enters the machine, it's accepted as an input and the machine produces an output f(x) according to the rule of the function. Thus we can think of the domain as the set of all possible inputs and the range as the set of all possible outputs.

함수를 기계로 생각하면 도움이 됩니다. x 가 함수 f 의 도메인에 있는 경우 x 가 기계에 들어가면 입력으로 받아 들여지고 기계는 함수의 규칙에 따라 출력 f(x) 를 생성합니다. 따라서 우리는 도메인을 가능한 모든 입력 집합으로 범위를 가능한 모든 출력 집합으로 생각할 수 있습니다.

The preprogrammed functions in a calculator are good examples of a function as a machine. For example, the square root key on your calculator computes such a function. You press the key labeled <math><semantics><msqrt><mrow/></msqrt></semantics></math> and enter the input x. If x < 0, then x is not in the domain of this function: that is, x is not an acceptable input, and the calculator will indicate an error. If x >= 0, then an approximation to <math><semantics><msqrt><mi>x</mi></msqrt></semantics></math> will appear in the display. Thus the <math><semantics><msqrt><mi>x</mi></msqrt></semantics></math> key on your calculator is not quite the same as the exact mathematical function f defined <math><semantics><mrow><mi>f</mi><mrow><mrow><mo fence="true" stretchy="false">(</mo><mrow><mi>x</mi></mrow><mo fence="true" stretchy="false">)</mo></mrow><mo stretchy="false">=</mo><msqrt><mi>x</mi></msqrt></mrow></mrow></semantics></math>.

계산기에서 사전에 프로그래밍된 함수는 기계 기능의 좋은 예입니다. 예를 들어, 계산기의 제곱근 키는 이러한 함수를 계산합니다. <math><semantics><msqrt><mrow/></msqrt></semantics></math> 라고 표시된 키를 누르고 x 를 입력하면, x < 0 인 경우, x 는 이 함수의 영역에 있지 않습니다. 즉 x 는 허용되는 입력이 아니기에 계산기는 오류를 나타냅니다. x >= 0 인 경우 <math><semantics><msqrt><mi>x</mi></msqrt></semantics></math> 에 대한 근사값이 디스플레이어 나타납니다. 따라서, 계산기의 <math><semantics><msqrt><mi>x</mi></msqrt></semantics></math> 키는 <math><semantics><mrow><mi>f</mi><mrow><mrow><mo fence="true" stretchy="false">(</mo><mrow><mi>x</mi></mrow><mo fence="true" stretchy="false">)</mo></mrow><mo stretchy="false">=</mo><msqrt><mi>x</mi></msqrt></mrow></mrow></semantics></math> 에 정의된 정확한 수학 함수 f와 완전히 다릅니다.

Another way to pricture a function is by an arrow diagram as in Figure 3. Each arrow connects an element of D to an element of E. The arrow indciates that f(x) is associated with x, f(a) is associated with a, and so on.

![Figure 3](/assets/images/20191226/Untitled1.png)

__Figure 3. Arrow diagram for f__

함수를 실행하는 다른 방법은 그림 3과 같이 화살표 다이어그램을 사용하는 것입니다. 각 화살표는 D의 요소를 E의 요소에 연결합니다. 화살표는 f(x)가 x 와 연관되어 있고 f(a) 는 a 와 연관되어 있음을 나타냅니다.

The most common method for visualizing a function is its graph. If f is a function with domain D, then its graph is the set of ordered pairs

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
 <semantics>
  <mrow>
   <mo stretchy="false">{</mo>
   <mrow>
    <mrow>
     <mrow>
      <mo fence="true" stretchy="false">(</mo>
      <mrow>
       <mrow>
        <mi>x</mi>
        <mi>,</mi>
        <mi>f</mi>
        <mrow>
         <mo fence="true" stretchy="false">(</mo>
         <mrow>
          <mi>x</mi>
         </mrow>
         <mo fence="true" stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mrow>
      <mo fence="true" stretchy="false">)</mo>
     </mrow>
     <mo stretchy="false">∨</mo>
     <mi>x</mi>
    </mrow>
    <mo stretchy="false">∈</mo>
    <mi>D</mi>
   </mrow>
   <mo stretchy="false">}</mo>
  </mrow>
 </semantics>
</math>

(Notice that these are input output pairs.) In other words, the graph of f consists of all points (x, y) in the coordinate plane such that y = f(x) and x is the domain of f.

함수를 시각화하는 가장 일반적인 방법은 그래프입니다. f 가 도메인 D의 함수인 경우 그래프는

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
 <semantics>
  <mrow>
   <mo stretchy="false">{</mo>
   <mrow>
    <mrow>
     <mrow>
      <mo fence="true" stretchy="false">(</mo>
      <mrow>
       <mrow>
        <mi>x</mi>
        <mi>,</mi>
        <mi>f</mi>
        <mrow>
         <mo fence="true" stretchy="false">(</mo>
         <mrow>
          <mi>x</mi>
         </mrow>
         <mo fence="true" stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mrow>
      <mo fence="true" stretchy="false">)</mo>
     </mrow>
     <mo stretchy="false">∨</mo>
     <mi>x</mi>
    </mrow>
    <mo stretchy="false">∈</mo>
    <mi>D</mi>
   </mrow>
   <mo stretchy="false">}</mo>
  </mrow>
 </semantics>
</math>

와 같은 순서쌍의 집합입니다. 다시 말해, f 의 그래프 y = f(x) 및 x 가 f 의 도메인이 되도록 좌표 평면의 모든 점 (x, y)으로 구성됩니다.

----

## MICROSERVICES IN THE APACHE KAFKA ECOSYSTEM

### ABSTRACT

This paper provides a brief overview of how microservices can be built in the apache kafka ecosystem. It begins with a look at event-driven services, exploring the tradeoffs involved in implementing the various protocols available for service to service communication. This is followed by a brief introduction of Confluent Enterprise, detailing a unique approach to meeting the challenges inherent in implementing and managing a services based environment.

이 백서는 아파치 카프카 생태계에서 마이크로 서비스를 구축하는 방법에 대한 간략한 개요를 제공합니다. 서비스 중심의 통신에 사용한 가능한 다양한 프로토콜을 구현하는데 관련된 장단점을 탐색하면서 이벤트 중심 서비스를 살펴보는 것으로 시작합니다. 그 다음에는 서비스 기반 환경 구현 및 관리에 내제된 과제를 해결하기 위해 고유한 접근 방식을 자세히 설명하는 Confluent Enterprise 에 대한 간력한 소개가 이어집니다.

### 1. MICROSERVICES BACKGROUND

When you build microservices in the apache kafka ecosystem you're blending two quite disparate worlds: the world of business systems (that is, of service based systems) and the world of stream processing. Historically, these two domains have been separate.

아파치 카프카 생태계에서 마이크로 서비스를 구축하려면 비즈니스 시스템 세계 (서비스 기반 시스템)와 스티림 처리 세계라는 두 개의 서로 다른 세계를 혼합하게 됩니다. 역사적으로 이 도메인은 분리되어 있었습니다.

Microservice architecture were originally designed to provide flexibility and adaptability to business systems, by splitting roles into lightweight, independently deployed services. Stream processing, on the other hand, addresses the problem of continually reacting to and processing data as it flows through a business.

마이크로 서비스 아키텍쳐는 원래 역할을 경량의 독립적으로 배포된 서비스로 분할하여 비즈니스 시스템에 유연성과 적응성을 제공하도록 설계되었습니다. 반면 스트림 처리는 데이터가 비즈니스를 통과할 때 지속적으로 반응하고 처리하는 문제를 해결합니다.

Despite originating in separate worlds, there is much crossover between these two domains. We can expect this to continue. As business systems have become increasingly data-centric, the links between different services have become denser and more unwieldy. This makes it hard to change and adapt such architectures as they grow. Simple request response protocols are quite limiting in this regard, but there are several ways to piece together service based systems. Brokered technology changes the dynamics of interaction by decoupling sender and receiver. The use of retentive broker technology and stream processing takes this even further, providing an alternate solution that better weathers the inevitable increase in service complexity.

별도의 세계애서 시작되었지만 두 도메인 간에는 많은 교차가 있습니다 우리는 이것이 계속될 것으로 기대할 수 있습니다. 비즈니스 시스템이 점점 더 데이터 줌심이 되어감에 따라 서로 다른 서비스 간의 연결이 조밀해지고 다루기 어려워졌습니다. 따라서 이러한 아키텍쳐가 성장함에 따라 변경 및 적용이 어렵습니다. 단순한 요청 응답 프로토콜은 이와 관련하여 상당히 제한적이지만 서비스 기반 시스템을 함께 구성하는 방법에는 여러가지가 있습니다. 중개 기술은 발신자와 수신자를 분리하여 상호 작용의 역할을 바꿉니다. 유지 브로커 기술과 스트림 처리를 사용하면 서비스 복잡성의 불가피한 증가를 더 잘 극복할 수 있는 대체 솔루션을 제공하는데 더 많은 시간이 소요됩니다.

Figure 1, below, shows how microservices can split the roles within a system into discrete units. These are often termed bounded contexts. Each of the services shown - UI, Order, Fulfillment etc - match to an underlying business function.

아래의 그림 1은 마이크로 서비스가 시스템 내 역할을 개별 장치로 분리하는 방법을 보여줍니다. 이를 경계 컨텍스트라고도 합니다. 표시되는 각 서비스(UI, 주문, 이행)는 기본 비즈니스 기능과 일치합니다.

![마이크로 서비스 예제](/assets/images/20191226/Untitled2.png)

Figure 1: A simple business system built on a microservices architecture

Microservice architectures come with a number of benefits. They provide a convenient abstraction for reuse. Breaking services up makes them more fungible. It provides the potential for increased scalability and fault tolerance. But the main benefit is that each service is deployed independently, and can evolve independently, of the others. This independence is key.

마이크로 서비스 아키텍쳐에는 여러 가지 이점이 있습니다. 재사용을 위한 편리한 추상화를 제공합니다. 서비스를 중단하면 더 재미있어 집니다. 확장성 및 내결합성이 향상될 수 있습니다. 그러나 주요 이점은 각 서비스가 독립적으로 배포되고 다른 서비스와 독립적으로 발전할 수 있다는 것입니다. 이 독립성이 핵심입니다.

The monolithic approach is quite different. A well managed monolithic system can provide several of the benefits named above: reuse, encapsulation, and some degree of scalability. But a the system grows, it becomes increasingly difficult to integrate the contributions of a radiply growing staff of engineers. Over time, a monolithic system struggles to provide the agility that business users typically need.

모놀리식 접근법은 상당히 다릅니다. 잘 관리되는 모놀리식 시스템은 재사용, 캡슐화 및 어느 정도의 확장성이라는 위에서 언급한 여러 가지 이점을 제공할 수 있습니다. 그러나 시스템이 성장함에 따라 급증하는 엔지니어 직원의 기여를 통합하는 것이 점점 어려워지고 있습니다. 시간이 지남에 따라 모놀리식 시스템은 비즈니스 사용자가 일반적으로 필요로 하는 민첩성을 제공하기 위해 고군분투합니다.

A second problem with monolithic architectures is that they typically assume some form of central, shared-state management, such as a central database.

----

## [OPENCV] FILE INPUT AND OUTPUT USING XML AND YAML FILES

Here we talk only about XML and YAML file inputs. You output (and its respective input) file may have only one of these extensions and the structure coming from this. They are two kinds of data structures you may serialize: mappings (like the STL map) and element sequence (like the STL vector). The difference between these is that in a map every element has a unique name through what you may access it. For sequences you need to go through them to query a specfic item.

여기서는 XML 및 YAML 파일 입력에 대해섲만 설명합니다. 출력 파일에는 이러한 확장 중 하나와 이로부터 오는 구조만 있을 수 있습니다. 직렬화할 수 있는 두 종류의 데이터 구조인 STL 맵과 같은 매핑과 STL 벡터와 같은 요소 시퀀스입니다. 이들의 차이점은 맵에서 모든 요소는 액세스 가능한 것을 통해 고유한 이름을 갖는다는 것입니다. 시퀀스의 경우 특정 항목을 쿼리하기 위해 시퀀스를 거쳐야 합니다.

1. XML/YAML File Open and Close

Before you write any content to such file you need to open it and at the end to close it. The XML/YAML data structure in OpenCV is cv::FileStorage. To specify that this structure to which file binds on your hard drive you can use either its constructor or the open() function of this:

```
string filename = "temp.xml";
FileStorage fs(filename, FileStorage::WRITE);
// ...
fs.open(filename, FileStorage::READ);
```

Either one of this you use the second argument is a constant specifying the type of operations you'll be able to on them: WRITE, READ or APPEND. The extension specified in the file name also determinates the output format that will be used. The output may be even compressed if you specify an extension such as `*.xml.gz`.

The file automatically closes when the cv::FileStorage objects is destroyed. However, you may explicitly call for this by using the release function:

이러한 파일에 내용을 쓰기 전에 파일을 열고 마지막에 닫아야 합니다. OpenCV 의 XML/YAML 데이터 구조는 cv::FileStorage 입니다. 하드 드라이브에서 파일을 바인딩하는 이 구조를 지정하려면 생성자 또는 다음의 open() 함수를 사용할 수 있습니다.

이 중 하나를 사용하면 두 번째 인수를 사용할 수 있습니다. 쓰기, 읽기 또는 추가와 같은 작업 유형을 지정하는 상수입니다. 파일 이름에 지정된 확장자는 또한 사용될 출력 형식을 결정합니다. `*.xml.gz` 와 같은 확장자를 저장하면 출력이 압축될 수도 있습니다.

cv::FileStorage 객체가 삭제되면 파일이 자동으로 닫힙니다. 그러나 release 함수를 사용하여 이를 명시적으로 호출할 수 있습니다.

2. INPUT/OUTPUT OF TEXT AND NUMBERS

The data structure uses the same `<<` output operator that the STL library. For outputting any type of data structure we need first to specify its name. We do this by just simply printing out the name of this. For basic types you may follow this with the print of the value:

```
fs << "iterationNr" << 100;
```

Reading in is a simple addressing (via the `[]` operator) and casting operation of a read via the `>>` operator:

```
int iterationNr;
fs["iterationNr"] >> iterationNr;
iterationNr = (int) fs["iterationNr"];
```

```
fs.release();
```

3. INPUT/OUTPUT OF OPENCV DATA STRUCTURE

Well these behave exactly just as the basic C++ types:

```
Mat R = Mat_<uchar>::eye(3, 3);
Mat T = Mat_<double>::zeros(3, 1);

fs << "R" << R;
fs << "T" << T;

fs["R"] >> R;
fs["T"] >> T;
```

4. INPUT/OUTPUT OF VECTORS (ARRAYS) AND ASSOCIATIVE MAPS

As I mentioned beforehand, we can output maps and sequences (array, vector) too. Again we first print the name of the variable and then we have to specify if our output is either a sequence or map.

For sequence before the first element print the "[" character and after the last one the "]" character:

```
fs << "strings" << "[";
fs << "image1.jpg" << "Awesomeness" << "baboon.jpg";
fs << "]";
```

For maps the drill is the same however new ew use the "{" and "}" delimiter characters:

```
fs << "Mapping";
fs << "{" << "One" << 1;
fs <<        "Two" << 2 << "}";
```

To read from these we use the cv::FileNode and the cv::FileNodeIerator data structures. The "[]" operator of the cv::FileStorage class returns a cv::FileNode data type. If the node is sequential we can use the cv::FileNodeIterator to iterate through th items:

```
FileNode n = fs["strings"];
if(n.type() != FileNode::SEQ)
{
  cerr << "strings is not a sequence !" << endl;
  return 1;
}

for(FileNodeIterator it = n.begin(); it != n.end()_; ++it)
{
  cout << (string)(*it) << endl;
}
```

For maps you can use the `[]` opeator again to access the given item (or the `>>` operator too):

```
n = fs["Mapping"];
cout << "Two" << (int)(n["Two"]) << "; ";
cout << "One" << (int)(n["One"]) << endl << endl;
```

5. READ AND WRITE YOUR OWN DATA STRUCTURES

Suppose you have a data structure such as:

```
class MyData
{
  public:   MyData() : A(0), X(0), id() {}
  public:   int A;
            double X;
            string id;
}
```

It's possible to serialize this through the OpenCV I/O XML/YAML interface (just as in case of the OpenCV data structures) by adding a read and a write function inside and outside of your class. For the inside part:

```
void write(FileStorage & fs) const
{
  fs << "{" << "A" << A << "X" << X << "id" << id << "}";
}

void read(const FileNode & node)
{
  A = (int) node["A"];
  X = (double) node["X"];
  id = (string) node["id"];
}
```

Then you need to add the following functions definitions outside the class:

```
void write(FileStorage & fs, const std::string &, const MyData & x)
{
  x.write(fs);
}

void read(const FileNode & node, MyData & x, const MyData & default_value = MyData())
{
  if(node.empty())
  {
    x = default_value;
  }
  else
  {
    x.read(node);
  }
}
```

Here you can observe that in the read section we defined what happens if the user tries to read a non existing node. In this case we just return the default initialization value, however a more verbose solution would be to return for instance a minus one value for an object id.

Once you added these four functions use the `>>` operator for write and the `<<` operator for read:

```
MyData m(1);
fs << "MyData" << m;
fs["MyData"] >> m;
```

Or to try out reading a non-existing read:

```
fs["NonExisting"] >> m;
cout << endl << "NonExisting = " << endl << m << endl;
```

### [OPENCV] HOW TO USE THE OPENCV `parallel_for_` TO PARALLELIZE YOUR CODE

#### PRECONDITION

The first precondition is to have OpenCV built with a parallel framework. In OpenCV 3.2, the following parallel frameworks are available in that order:

1. Intel Threading Building Blocks (3rd party library, should be explicitly enabled)
2. C = Parallel C/C++ Programming Language Extension (3rd party library, should be explicitly enabled)
3. OpenMP (integrated to compiler, should be explicitly enabled)
4. APPLE GCD (system wide, used automatically (APPLE only))
5. Windows RT concurrency (system wide, used automatically (Windows RT only))
6. Windows concurrency (part of runtime, used automatically (Windows only - MSVC++ > 10))
7. Pthreads (if available)

As you can see, several parallel frameworks can be used in the OpenCV library. Some parallel libraries are third party libraries and have to explicitly built and enabled in CMake (e.g. TBB, C=), others are automatically available with the platform(e.g. APPLE GCD) but chances are that you should be enable to have access to a parallel framework either directly or by enabling the option in CMake and rebuild the library.

The second (weak) precondition is more related to the task you want to achieve as not all computations are suitable / can be adatapted to be run in a parallel way. To remain simple, tasks that can be split into multiple elementary operations with no memory dependency (no possible race condition) are easily parallelizable. Computer vision processing are often easily parallelizable as most of the time the processing of one pixel does not depend to the state of other pixels.

첫번째 전제 조건은 OpenCV 를 병렬 프레임워크로 빌드하는 것입니다. OpenCV 3.2 에서는 다음과 같은 병렬 프레임 워크를 순서대로 사용할 수 있습니다.

1. Intel Threading Building Block
2. C = Parallel C/C++ 프로그래밍 확장
3. OpenMP
4. Apple GCD
5. Windows RT concurrency
6. Windows concurrency
7. PThread

보시다시피 OpenCV 라이브러리에서 여러 병렬 프레임 워크를 사용할 수 있습니다. 일부 병렬 라이브러리는 타사 라이브러리이며 CMake 에서 명시적으로 빌드하고 활성화해야 합니다. 다른 라이브러리는 플랫폼에서 자동으로 사용할 수 있습니다. 직접 혹은 CMake 옵션을 활성화하고 라이브러리를 다시 빌드하여 병렬 프레임워크를 활성화할 수 있습니다.

두번째 전제 조건은 계산이 병렬 방식으로 실행되도록 데이터 처리할 수 있도록 해야 합니다. 간단하게 메모리 의존성없이 여러 개의 기본 작업으로 부할할 수 있는 작업은 쉽게 병렬화할 수 있습니다. 컴퓨터 비전 프로세싱은 종종 한 픽셀의 프로세싱이 다른 픽셀위 상태에 의존하지 않기 때문에 쉽게 병렬화 될 수 있습니다.

#### SIMPLE EXAMPLE: DRAWING A MANDELBROT SET

We will use the example of drawing a madelbrot set to show how from a regular sequential code you can easily adapt the code to parallelize the computation.

Mandelbrot 세트를 그리는 예제를 사용하여 순차 코드에서 쉽게 계산을 병렬화하는 방법을 보여줍니다.

#### THEORY

The Mandelbrot set definition has been named in tribute to the mathematician Benoit Mandelbrot by the mathematician Adrien Douady. It has been famous outsize of the mathematics field as the image representation is an example of a class of fractals, a mathematical set that exhibits a repeating pattern displayed at every scale (even more, a Mandelbrot set is self similar as the whole shape can be repeatedly seen at different scale). For a more in depth introduction, you can look at the corresponding Wikipeida article. Here, we will just introduce the formula to draw the Mandelbrot set (from the mentioned Wikipedia article).

Mandelbrot 세트 정의는 수학자 Adrien Douady 에 의해 수학자 Benoit Mandelbrot 에게 경의를 표했습니다. 이미지 표현은 모든 종류의 스케일에서 반복되는 패턴을 나타내는 수학적 세트인 프랙탈 클래스의 예이기 때문에 수학 분야보다 규모가 큰 것으로 유명합니다. (심지어 만델브로트 세트는 전체 모양과 비슷합니다.) 더 깊이 소개하려면 해당 위키피이더 기사를 볼 수 있습니다. 여기서는 언급된 위키피디아 기사에서 Mandelbrot 세트를 그리는 공식을 소개합니다.

----

The mandelbrot set of values of c in the complex plane for which the orbit of 0 under iteration of the quadratic map

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
 <semantics>
  <mtable>
   <mtr>
    <mtd>
     <mrow>
      <msub>
       <mi>z</mi>
       <mn>0</mn>
      </msub>
      <mo stretchy="false">=</mo>
      <mn>0</mn>
     </mrow>
    </mtd>
   </mtr>
   <mtr>
    <mtd>
     <mrow>
      <msub>
       <mi>z</mi>
       <mrow>
        <mi>n</mi>
        <mo stretchy="false">+</mo>
        <mn>1</mn>
       </mrow>
      </msub>
      <mo stretchy="false">=</mo>
      <mrow>
       <msup>
        <msub>
         <mi>z</mi>
         <mi>n</mi>
        </msub>
        <mn>2</mn>
       </msup>
       <mo stretchy="false">+</mo>
       <mi>c</mi>
      </mrow>
     </mrow>
    </mtd>
   </mtr>
  </mtable>
 </semantics>
</math>

remained bounded. That is, a complex number c is part of the Mandelbrot set if, when starting with <math><semantics><mrow><msub><mi>z</mi><mn>0</mn></msub><mo stretchy="false">=</mo><mn>0</mn></mrow></semantics></math> and applying the iteration repeatedly, the absolute value of <math><semantics><msub><mi>z</mi><mi>n</mi></msub></semantics></math> remains bounded however large n gets. This can also be represented as

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
 <semantics>
  <mrow>
   <mrow>
    <munder>
     <mi>lim</mi>
     <mrow>
      <mi>n</mi>
      <mo stretchy="false">→</mo>
      <mi mathvariant="normal">∞</mi>
     </mrow>
    </munder>
    <mrow>
     <mo fence="true" stretchy="false">|</mo>
     <mrow>
      <msub>
       <mi>z</mi>
       <mrow>
        <mi>n</mi>
        <mo stretchy="false">+</mo>
        <mn>1</mn>
       </mrow>
      </msub>
     </mrow>
     <mo fence="true" stretchy="false">|</mo>
    </mrow>
   </mrow>
   <mo stretchy="false">⩽</mo>
   <mn>2</mn>
  </mrow>
 </semantics>
</math>

#### PSEUDOCODE

A simple algorithm to generate a representation of the Mandelbrot set is called the "escape time algorithm". For each pixel in the rendered image, we tst using the recurrence relation if the complex number is bounded or not under a maximum number of iterations. Pixels that do not belong to the Mandelbrot set will escape quickly whereas we assume that the pixel is in the set after a fixed maximum number of iterations. A high value of iterations will produce a more detailed image but computation time will increase accordingly. We use the number of iterations needed to "escape" to depict the pixel value in the image.

Mandelbrot 세트의 표현을 생성하는 간단한 알고리즘을 "escape time algorithm"이라고 합니다. 렌더링 된 이미지의 각 픽셀에 대해 복소수가 최대 반복 횟수 이하인지 여부에 따라 반복 관계가 사용됩니다. Mandelbrot 세트에 속하지 않은 픽셀은 빠르게 빠져 나가지만 고정된 최대 반복 횟수 후에 픽셀이 세트에 있다고 가정합니다. 반복 값이 높을수록 더 자세한 이미지가 생성되지만 그에 따라 계산 시간이 늘어납니다. 이미지의 픽셀 값을 나타내기 위해 탈출하는데 필요한 반복 횟수를 사용합니다.

```
For each pixel (Px, Py) on the screen, do:
{
  x0 = scaled x coordinate of pixel (scaled to lie in the Mandelbrot X scale (-2, 1))
  y0 = scaled y coordinate of pixel (scaled to lie in the Mandelbrot Y scale (-1, 1))
  x = 0.0
  y = 0.0
  iteration = 0
  max iteration = 1000
  while (x * x + y * y < 2 * 2 and iteration < max iteration) {
    xtemp = x * x - y * y + x0
    y = 2 * x * y + y0
    x = xtemp
    iteration = iteration + 1
  }
  color = palette[iteration]
  plot(Px, Py, color)
}
```

To relate between the pseudocode and the theory, we have:

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
 <semantics>
  <mrow>
   <mi>z</mi>
   <mo stretchy="false">=</mo>
   <mrow>
    <mi>x</mi>
    <mo stretchy="false">+</mo>
    <mi mathvariant="italic">iy</mi>
   </mrow>
  </mrow>
 </semantics>
</math>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
 <semantics>
  <mrow>
   <mrow>
    <msup>
     <mi>z</mi>
     <mn>2</mn>
    </msup>
    <mo stretchy="false">=</mo>
    <mrow>
     <msup>
      <mi>x</mi>
      <mn>2</mn>
     </msup>
     <mo stretchy="false">+</mo>
     <mi>i</mi>
    </mrow>
   </mrow>
   <mn>2</mn>
   <mrow>
    <mi mathvariant="italic">xy</mi>
    <mo stretchy="false">−</mo>
    <msup>
     <mi>y</mi>
     <mn>2</mn>
    </msup>
   </mrow>
  </mrow>
  <annotation encoding="StarMath 5.0">z ^ 2 = x ^ 2 + i2xy - y ^ 2</annotation>
 </semantics>
</math>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
 <semantics>
  <mrow>
   <mrow>
    <mi>c</mi>
    <mo stretchy="false">=</mo>
    <mrow>
     <msub>
      <mi>x</mi>
      <mn>0</mn>
     </msub>
     <mo stretchy="false">+</mo>
     <mi>i</mi>
    </mrow>
   </mrow>
   <msub>
    <mi>y</mi>
    <mn>0</mn>
   </msub>
  </mrow>
  <annotation encoding="StarMath 5.0">c = x _ 0 + i y _ 0</annotation>
 </semantics>
</math>

![picture](/assets/images/20191226/9906EE3359BF835C18.png)

On this figure, we recall that the real part of a complex number is on the x axis and the imaginary part on the y axis. You can see that the whole shape can be repeatedly visible if we zoom at particular locations.

이 그림에서 복소수의 실수 부분은 x 축에 있고, 허수부분은 y 축에 있습니다. 특정 위치에 확대하면 전체 모양을 반복해서 볼 수 있습니다.

#### IMPLEMENTATION

##### ESCAPE TIME ALGORITHM IMPLEMENTATION

```
int mandelbrot(const complex<float> & _z, const int max)
{
  complex<float> z = _z;
  for(int t = 0; t < max; t++)
  {
    if(z.real() * z.real() + z.imag() * z.imag() > 4.0f)
    {
      return t;
    }
    z = z * z + _z
  }
  return max;
}
```

Here, we used the `std::complex` template class to represent a complex number. This function performs the test to check if the pixel is in set or not and returns the escaped iteration.

여기에서는 `std::complex` 텝플릿 클래스를 사용하여 복소수를 나타냈습니다. 이 함수는 테스트를 수행하여 픽셀이 설정되어 있는지 여부를 확인하고 "이스케이프된" 반복을 반환합니다.

##### SEQUENTIAL MANDELBROT IMPLEMENTATION

```
void sequential_mandelbrot(Mat & img, const float _x, const float _y, const float scale_x, const float scale_y)
{
  for(int i = 0; i < img.rows; i++)
  {
    for(int j = 0; j < img.cols; j++)
    {
      float x = j / scale_x + _x;
      float y = i / scale_y + _y;

      complex<float> z(x, y);
      uchar value = (uchar) mandelbrotFormula(z);
      img.ptr<uchar>(i)[j] = value;

    }
  }
}
```

In this implementation, we sequentially iterate over the pixels in the rendered image to perform the test to check if the pixel is likely to belong to the mandelbrot set or not.

이 구현에서는 렌더링된 이미지의 픽셀을 순차적으로 반복하여 픽셀이 Mandelbrot 세트에 속하는지 여부를 확인하는 테스트를 수행합니다.

Another thing to do is to transform the pixel coordinate into the Mandelbrot set space with:

또 다른 방법은 픽셀 좌표를 Mandelbrot 설정 공간으로 변환하는 것입니다.

```
Mat mandelbrotImg(4800, 5400, CV_8U);
float x[2] = { -2.1f, 0.6f };
float y[2] = { -1.2f, 1.2f };
float scale_x = mandelbrotImg.cols / (x[1] - x[0]);
float scale_y = mandelbrotImg.rows / (y[1] - y[0]);
```

Finally, to assign the grayscale value to the pixels, we use the following rule:

- a pixel is black if it reaches the maximum number of iterations (pixel is assumed to be in the Mandelbrot set).
- otherwise we assign a grayscale value dependin on the escaped iteration and scaled to fit the gray scale range.

마지막으로 회색조 값을 픽셀에 할당하기 위해 다음 규칙을 사용합니다.

- 픽셀이 최대 반복 횟수에 도달하면 검은색입니다.
- 그렇지 않으면 탈출된 반복에 따라 그레이 스케일 값을 할당하고 그레이 스케일 범위에 맞게 스케일링됩니다.

```
int mandelbrotFormula(const complex<float> & z, const int max = 500)
{
  int value = madelbrot(z, max);
  if(max - value == 0)
  {
    return 0;
  }
  return cvRound(sqrt(value / (float) max) * 255);
}
```

Using a linear scale transform is not enough to perceive the gray scale variable. To overcome this, we will boost the perception by using a square root scale transformation (borrowed from Jeremy D. Frens in his blog post):

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mrow>
     <mo fence="true" stretchy="false">(</mo>
     <mrow>
      <mi>x</mi>
     </mrow>
     <mo fence="true" stretchy="false">)</mo>
    </mrow>
    <mo stretchy="false">=</mo>
    <mrow>
     <msqrt>
      <mrow>
       <mo fence="true" stretchy="false">(</mo>
       <mrow>
        <mrow>
         <mi>x</mi>
         <mo stretchy="false">/</mo>
         <mi mathvariant="italic">max</mi>
        </mrow>
       </mrow>
       <mo fence="true" stretchy="false">)</mo>
      </mrow>
     </msqrt>
     <mo stretchy="false">×</mo>
     <mn>255</mn>
    </mrow>
   </mrow>
  </mrow>
  <annotation encoding="StarMath 5.0">f(x) = sqrt( x / max ) times 255</annotation>
 </semantics>
</math>

![picture](/assets/images/20191226/how_to_use_OpenCV_parallel_for_sqrt_scale_transformation.png)

선형 스케일 변환을 사용하는 것만으로는 그레이 스케일 변화를 인식할 수 없습니다. 이를 극복하기 위해 제곱근을 이용하여 스케일 변환을 사용하여 인식을 향상 시킬 것입니다.

The green curve corresponds to a simple linear scale transformation, the blue one to a square root scale transformation and you can observe how the lowest values will be boosted when looking at the slope at these positions.

녹색 곤선은 단순한 선형 스케일 변환에 해당하고 파란색은 제곱근을 취한 스케일 변환에 해당하며 이러한 위치에서 기울기를 볼 때 가장 낮은 값이 어떻게 향상되는지 관찰할 수 있습니다.

#### PARALLEL MANDELBROT IMPLEMENTATION

When looking at the sequential implementation, we can notice that each pixel is computed independently. To optimize the computation, we can perform multiple pixel calculations in parallel, by exploiting the multi-core architecture of modern processor. To achieve this easily, we will use the OpenCV `cv::parallel_for_` framework.

```
class ParallelMandelbrot : public ParallelLoopBody
{
public: ParallelMandelbrot (Mat & img, const float x, const float y, const float scale_x, const float scale_y)
          : m_img(img), m_x(x), m_y(y), m_scale_x(scale_x), m_scale_y(scale_y)
        {

        }
public: virtual void operator()(const Range & range) const CV_OVERRIDE
        {
          for(int i = range.start; i < range.end; i++)
          {
            int integer = i / m_img.cols;
            int remain = i % m_img.cols;

            float x = remain / m_scale_x + m_x;
            float y = integer / m_scale_y + m_y;

            complex<float> z(x, y);
            uchar value = (uchar) mandelbrotFormula(z);
            m_img.ptr<uchar>(i)[j] = value;
          }
        }
public: ParallelMandelbrot & operator=(const ParellelMandelbrot &)
        {
          return *this;
        }
private:  Mat & m_img;
private:  float m_x;
private:  float m_y;
private:  float m_scale_x;
private:  float m_scale_y;
};
```

The first thing is to declare a custom class that inherits from `cv::ParallelLoopBody` and to override the `virtual void operator()(const cv::Range & range) const`.

The range in the `operator()` represents the subset of pixels that will be treated by an individual thread. This splitting is done automatically to distributed equally the computation load. We have to convert the pixel coordinate to a two dimension \[row, col\] coodniate. Also note that we have to keep a reference on the mat image to be able to modify in place the image.

첫번째는 `cv::ParallelLoopBody` 에서 상속하여 사용자 정의 클래스를 선언하고 `virtual void operator()(const cv::Range & range) const` 를 재정의하는 것입니다.

연산자의 파라미터인 range 는 개별 스레드에서 처리할 픽셀의 하위 집합을 나타냅니다. 이 분할은 계산 로드를 균등하게 분배하기 위해 자동으로 수행됩니다. 픽셀 인덱스 좌표를 2차원 \[행, 열\] 좌표로 변환해야 합니다. 또한 이미지를 제자라에 수정할 수 있으려면 매트 이미지에 대한 참조를 유지해야 합니다.

The parallel execution is called with:

```
ParallelMandelbrot parallelMandelbrot(mandelbrotImg, x, y, scale_x, scale_y);
parallel_for_(Range(0, mandelbrotImg.rows * mandelbrotImg.cols), parallelMandelbrot);
```

Here, the range represents the total number of operations to be executed, so the total number of pixels in the image. To set the number of threads, you can use `cv::setNumThreads`. You can also specify the number of splitting using the nstripes parameter in `cv::parallel_for_`. For instance, if your processor has 4 threads, settin cv::setNumThreads(2) or setting nsripes=2 should be the same as by default it will use all the processor threads available but will split the workload only on two threads.

여기서 범위는 실행될 총 작업 수를 나타내고, 이것은 이미지의 총 픽셀 수입니다. 스레드 수를 설정하려면 다음을ㄹ 사용하시면 됩니다. `cv::setNumThreads,` `cv::parallel_for_` 에서 nstripes 매개 변수를 사용하여 분활 수를 지정할 수도 있습니다. 예를 들어, 프로세스에 4개의 스레드가 있는 경우 `cv::setNumThreads(2)` 또는 nstripes = 2 를 설정하여 기본적으로 사용 가능한 모든 프로세스 스레드를 사용하지만 정확하게 두 개의 스레드로만 작업 부하를 분할하는 것과 동일하게 동작합니다.

> NOTE

C++ 11 표준에서 람다로도 구현할 수 있습니다.

```
parallel_for_(Range(0, mandelbrotImg.rows * mandelbrotImg.cols), [&](const Range & range){
  for(int r = range.start; r < range.end; r++)
  {
    int i = r / mandelbrotImg.cols;
    int j = r % mandelbrotImg.cols;

    float x0 = j / scaleX + x1;
    float y0 = i / scaleY + y1;

    complex<float> z0(x0, y0);
    uchar value = (uchar) mandelbrotFormula(z0);
    mandelbrotImg.ptr<uchar>(i)[j] = value;
  }
});
```

### RESULT

You can find the full tutorial code here. The performance of the parallel implementation depends of the type of CPU you have. For instance, on 4 cores / 8 threads CPU, you can expect a speed up of around 6.9X. There are many factors to explain why we do not achieve a speed up of almost 8X. Main reason is should be mostly due to:

- the overhead to create and manage the threads.
- background processes running int parallel
- the difference between 4 hardware cores with 2 logical threads for each core and 8 hardware cores.

The resulting image produces by the tutorial code (you can modify the code the use more interations and assign a pixel color depending on the escaped iteration and using a color palette to get more aesthetic images):

전체 튜토이얼 코드는 여기에서 찾을 수 있습니다. 병렬 구현의 성능은 사용중인 CPU 유형에 따라 다릅니다. 예를 들어 4 코어 / 8 스레드 CPU 에서 약 6.9 배의 속도 향상을 기대할 수 있습니다. 거의 8 배의 속도 향상을 달성하지 못한 이유를 설명할 요소가 많이 있습니다. 주요 이유는 주로 다음과 같은 이유 때문입니다.

- 스레드를 생성하고 관리하는 오버헤드
- 병렬로 실행되는 백그라운드 프로세스
- 각 코어당 2 개의 논리 스레드가 있는 4 개의 하드웨어 코어와 8개의 하드웨어 코어의 차이점

학습서 코드에서 생성된 결과 이미지 (더 많은 반복을 사용하도록 코드를 수정하고 이스케이프된 반복에 따라 픽셀 색상을 지정하고 색상 팔레트를 사용하여 더 심미적인 이미지를 얻을 수 있습니다.)

## 컴퓨터 비전

최초의 사진이 등잔한 이후 120년 동안 아무도 영상에서 정보를 자동으로 추출하려는 노력을 하지 않았다. 하지만 1940년대에 디지털 컴퓨터가 등장한 이후 상황은 급변한다. 2.0 을 입력하면 2.0 의 제곱근이 순식간에 계산하여 1.414213562 를 출력하는 신기한 기계를 갖게 되자. 영상을 입력하면 자동으로 해석하여 정보를 출력하는 기계를 상상하게 된 것이다.

불과 수십 년이 지난 현재 그와 같은 일이 가능해졌다. 고글이라는 스프트폰 앱에서 와인 병을 찍으면 라벨을 인식하여 관련 웹 사이트로 안내한다. 와인 병뿐 아니라 책 표지나 유용한 랜드마크 건물도 인식할 수 있다. 해상도가 1,000 x 1,000 이상인 카메라, 1GHz 가량의 CPU, 그리고 16 GB 상당의 기억장치로 무장한 기계가 물체를 인식하는 프로그램을 갖추고 주머니 속에 들어 있는 것이다. XBox 게임기에는 사람의 동작을 인식하는 기능이 들어 있다. 관절 20군데를 실시간으로 추적하여 동작을 인삭하므로 사용자는 아무런 장치를 부착하지 않은 채 게임을 즐길 수 있다. 이와 같이 컴퓨터 비전은 이미 사람의 주머니 속이나 가정의 거실에 들어왔다. 컴퓨터 비전의 응용은 급속도로 팽창하고 있다.

이러한 일이 가능하기까지 여러 번의 난관이 있었다. 역설적으로 컴퓨터의 성능이 미약했던 1950년 ~ 1960 년대에는 사람의 시각에 필적하는 컴퓨터 비전 시스템을 만들려는 야심에 찬 목표가 대세였다. 하지만 오래 지나지 않아 무모한 목표임을 깨닫고, 특수한 환경에서 특정 임무를 수행하는 시스템을 구현하는 실용적인 목표로 전환하였다. 인공지능도 아주 비슷한 길을 걸었다. 1950년대에 사람에 맞먹는 지능 기계를 만들려는 목표로 시작한 연구는 1970년대 중반에 'AI의 겨울'이라 불리는 침체기에 맞는다. 하지만 1980년대에 특수 용도의 전문 시스템의 성공으로 부활하게 되고, 특수 용도의 지능 시스템을 만드는 연구가 지금까지 활발하게 진행중이다.

고글 앱과 XBox 게임기 이 두 시스템은 이러한 실용적인 목표를 달성한 대표적인 예이다. 고글은 와인 라벨이 화면 중앙에 오도록 신경을 써서 찍어야 하며, XBox 는 적당한 거리에서 한정된 동작을 하는 경우에만 제대로 인삭한다. 그렇지만 주어진 임무를 훌륭하게 수행하는 매우 유용한 시스템임을 부인할 수 없다.

컴퓨터 비전의 응용은 꽃피고 있으며 그 범위는 빠르게 팽창하고 있따. 때로는 문제와 응용이 혼란을 빚는 경우가 있다. 예를 들어, 어떤 책은 얼굴 인식을 컴퓨터 비전의 응용으로 내세우기도 하는데, 얼굴 인식은 응용이 아니라 컴퓨터 비전이 풀어야 하는 문제이다. 이러한 혼란이 일어나는 이유는 문제 해결 과정이 계층적인 탓이 크다. 예를 들어, 얼굴 인식에 PCA 라는 특징 추출 연산을 활용하므로 얼굴 인식은 PCA의 응용이라는 식으로 범위를 한정하는 것은 적절하다. 주요 응용 분야를 정리해보면 다음과 같다.

- 오락: 사람의 동작을 인삭하는 기능은 게임이나 가전을 편안하게 제어할 수 있도록 인터페이스로 사용된다. 최근 이러한 종류의 인터페이스를 그래픽 사용자 인터페이스와 구별하기 위해 자연스런 사용자 인터페이스(Natural User Interface) 또는 자연스런 인터페이스라고 부른다. 아바타와 타이타닉에서와 같이 실사 영상과 컴퓨터 그래픽스로 제작된 영상을 합성하여 실물감이 뛰어난 영화의 한 장면을 제작하기도 한다.

- 교통: 자동차의 자율 주차 기능은 이미 상품화되어 있고, 자율 주행 자동차ㅗ 시험 운행을 성공적으로 마치고 도로를 달리고 있다. 운전자의 졸음 상태를 인식하고 경고를 울리는 서비스도 상품화 단계에 들어섰다. 번호판을 인식하여 과속 단속 또는 주차 관리하는 제품은 이미 널리 사용된다.

- 보안 - 공황 검색대에서 짐을 검사하는 비전 시스템은 오래 전부터 사용해 왔으며, 그 기능은 점차 발전하는 중이다. 감시용 카메라는 도로, 골목, 학교, 관공서 등 도처에 설치되어 있다. 현재 단순히 녹화 기능만 사용하고, 필요한 경우에만 되돌려보는 용도로 이용하지만 점점 컴퓨터 비전을 적용하여 이상 징후를 인지하고 자동으로 알려주는 기능이 추가될 것이다. 얼굴, 지문, 홍체 인삭을 이용한 보안 제품인 이미 시중에서 쉽게 찾아볼 수 있고, 경찰이 범인을 추적할 때 꼭 필요한 장비 중 하나이다.

- 산업: 공장 자동화는 컴퓨터 비전을 가장 먼저 적용한 분야이다. 그만큼 생산 현장에서 자동화에 대한 욕구가 강하다는 반증이다. 칩, 기계부품, LED 와 같은 생산 제품의 검사, 조립, 페인트 칠, 납땜과 같은 작업에 주로 사용한다. 몸통이 고정된 매니퓰레이터라 불리는 로봇뿐 아니라 바퀴가 달린 이동용 로봇에도 컴퓨터 비전 기능이 적용된다.

- 계산 사진학(computational photography)
  다른 응용에 비해 비교적 새로운 응용 분야이다. 여러 장의 사진을 찍어 그것들을 이어 붙여 파노라마 영상을 제작하는 기능이 이미 디지털 카메라에 내장되어 있따. 또한 적정 노출에 실패한 사진의 복원, 사진에서 물체를 오려내어 다른 사진의 배경에 붙이는 작업, 흑백 영화를 컬로로 변환하는 작업 등 많은 곳에 응용된다.

- 의료: 연구 개발비가 가장 많이 투입되고 고가의 장비를 사용하느 분야이다. 여러 장의 2차원 단층 영상으로부터 3차원 영상을 생성하는 기능이 대표적인 예이다. 이외에도 정교하게 수술할 수 있도록 수술 부위를 찾아 안내하는 수술용 로봇도 있다. 혈액 샘풀을 보고 단위 면적당 적혈구의 수를 세거나, 특정 세포의 움직임을 추적하여 활동성을 추정하느 작업등에 응용된다.

- 과학: 화성에 착륙한 우주선은 울퉁불퉁한 화성 표면을 이리저리 다니며 사진을 찍고 토양 샘풀을 채취하는 등의 임무를 수행한다. 이때 웅덩이에 빠지거나 돌부리에 걸려 넘어지지 않으려면 컴퓨터 비전의 도움을 받아야 한다. 나노 과학에서는 전자 현미경으로 찍은 영상을 분석하여 나노 제품의 품질을 분석한다.

- 농업: 사과나 딸기 같은 농산품을 기계 장치에 넣어 분류할 때 컴퓨터 비전을 사용하면 좀더 정확하고 빠를 뿐 아니라, 비접촉으로 훼손 없이 분류 작업을 마칠 수 있다. 농산물이 자라는 환경을 감시하는 데에도 효과적으로 활용된다.

- 군사: 의료 못지않게 많은 연구 개발비가 투입되는 분야로, 컴퓨터 비전의 초창기 연구비를 안정적으로 지원하였다. 다리가 네 개인 군사용 로못 빅독은 짐을 지고 시속 6.4 km 로 달릴 수 있는 스테레오 비전 기능을 갖추고 있다. 앞으로 비전 기능을 갖추고 전투 현장을 누비는 전투용 로못이 등장한다면 전쟁의 승리는 군인의 의지와 전투력이 아닌 로못 기술이 좌우할 것이다.

- 모바일: 관광 안내용이나 교육용으로 제작된 증강현실 스마트폰 앱이 증가하고 있다. 파노라마 영상을 만들어 주는 포토신스, 나뭇잎을 찍으면 어떤 식물인지 알려주는 리프스냅, 책 표지, 와인 라벨, 유명 건물 등을 인식하는 고글 앱이 대표적이다.

## 2. 컴퓨터 비전 문제는 어떻게 해결하나?

사람은 아무 어려움 없이 순식간에 영상을 해석하고 이해한다. 포수는 투수가 던진 공을 추적하여 정확히 받아내며, 등산객은 가파른 지형을 인식하여 거리낌 없이 산을 오르내린다. 사람의 시각은 빠르고 강건하다. 강건이라는 말은 어떤 상황에 처하더라도 비슷한 성능을 발휘한다는 뜻이다. 컴퓨터로 사람 수준의 성능을 발휘하는 것은 현재 기술로는 불가능하다. 이러한 어려움은 왜 생길까? 그럼에도 불구하고 실용적인 시스템은 어떻게 만드는 것일까?

### 2.1. 과학적 접근과 공학적 접근

컴퓨터 비전의 목표는 다음과 같이 두 가지로 구분해볼 수 있다.

- 사람의 시각에 맞먹는 인공 시각을 만든다.
- 한정된 범위에서 특정한 임무를 달성하는 인공 시각을 만든다.

그림은 컴퓨터 비전의 첫번째 목표에 해당하는 예를 보여주는데, 어떤 영상이 입력되더라도 내용을 분석하고 해석하고 이해한 후 그 결과를 출력해야 한다. 맨 왼쪽 영상의 경우 '육상 경기에 멀리뛰기를 하는 여자 선수' 정도의 정보를 알려줄 것이다. 더 자세한 정보가 필요하 경우, 손과 발의 동작이라든가 얼굴 표정까지 기술할 수 있다. 육상에 조예가 있는 사람이라면 자세를 보고 어느 정도의 성적을 거둘 것인지도 유추할 수 있을 것이다. 이것이 사람의 인지 능력이다. 컴퓨터 비전이 첫번째 목표를 달성하기 위해서는 필연적으로 인간의 시각이 어떻게 작동하는지 원리를 밝혀낸 다음, 컴퓨터로 모방해야 한다. 이는 '과학적인 접근' 방법으로, 현재 뇌 과학의 주된 관심사 중 하나이다. (사람의 시각은 10억년 진화의 결과이다. 사람의 뇌 속에 약 10 ^ 11 개의 뉴런이 약 10^14 개의 시냅스로 서로 연결되어 있다는 기초적인 사실뿐 아니라 시각 처리 과정에 관한 많은 사실이 밝혀져 있따.) Marr 로 대표되는 초창기 연구자들이 취했던 방식은 첫번째 목표에 가깝다. 이와 같은 과학적 접근 방법은 크게 두 가지의 어려움을 안고 있다. 인간의 시각 과정을 완전히 밝혀낼 수 있나? 그것을 컴퓨터로 시뮬레이션을 할 수 있나? 현재 기술로는 불가능하다. 인간의 지능을 구성하는 지식 표현, 학습, 추런, 창작 등에 해당하는 인공 지능 문제가 모두 풀려야 가능할 것으로 보인다. (인공지능 완비라는 용어가 있다. 인공지능을 구성하는 여러 가지 문제 중 하나라도 풀리면 나머지 문제도 풀린다는 의미이다.) 하지만 어느 것 하나 제대로 풀리지 않고 있다.

> 과학적 접근 방법에 관심있는 독자는 12장을 먼저 읽어 보기 바란다. 사람 시각과 연관된 연구를 추천한다.

컴퓨터 비전이 어려운 본질적인 이유는 역문제(inverse problem)라는 점에서 찾아볼 수 있다. 입력 영상은 3차원 세계를 투영하여 얻은 것인데, 컴퓨터 비전은 2차원 영상으로부터 역으로 3차원 세계를 알아내는 과정이다. 불행히도 투영 과정에서 물체까지의 거리와 같은 중요한 정보는 사라진다. 하지만 사람은 영상을 보면 필요한 3차원 정보를 매우 쉽게 추론해 낸다. 이런 점에 희망을 걸어 볼 수 있지만, 현재는 그런 추론 기능을 갖는 알고리즘은 없다. 어려움을 야기하는 또 다른 이유는 컴퓨터 비전이 풀어야 하는 문제 대부분이 불량(ill-posed problem) 라는 점이다. (에를 들어 정렬은 우량 문제이다. - well posed problem 그 이유는 답이 유일하기 때문이다.) 즉 문제의 답이 유일하지 않다. 예를 들어, 영역 분할 문제를 생각해보자. 사람 몸 전체를 하나의 영역으로 분할할 수 있지만, 얼굴, 몸통, 다리, 팔을 각기 다른 영역으로 분할할 수도 있다. 또 다른 어려움은 여러 변형이 동시에 일어나기 때문에 발생한다. 크기, 회전, 투영이라는 기하학적 변환(geometric transformation) 뿐만 아니라 조명, 그림자 등의 광도 변화(photometric transformation)이 동시에 얼아난다. 또한 영상 획득 과정에서 발생하는 여러 종류의 광학적 잡은도 어려움의 원인이 된다.

지금 당장 풀기 어려우면 우회하는 수밖에 없다. 그림은 컴퓨터 비전의 두번째 목표를 설명하는 사례이다. 이들 영상은 대략 일정한 위치와 방향에서 찍은 것으로, 자동차 번호판을 인식해 치리하는 특정 임무가 주어져 있다. '공학적인 접근' 방법에 해당하며, 첫번째 목표에 비해 달성하기 쉽다. 번호판을 검춣하기(detection)위해서는 직사각형 모양을 찾으면 되고, 글자의 배열을 알고 있으므로 글자 인식도 가능핟. 굳이 인간의 시각 처리 원리를 모방하지 않아도 얼마든지 주어진 문제를 푸는 알고리즘을 고안할 수 있다. 현재 컴퓨터 비전은 대부분 두번째 목표를 추구한다. 이 책에서 제시하는 대부분의 알고리즘도 과학적 접근 방법인 첫번째 목표보다는 공학적 접근 방법인 두번째 목표에 가깝다.

공학적인 접근을 취하더라도 우수한 거능을 원한다면 그리 만만한 문제는 아니다. 왜냐하면 과학적인 접근 방법에서 언급했던 어려움과 원인이 사라진 것이 아니라 단지 정도만 줄어들었기 때문이다. 여전히, 역문제, 불량 문제이고 어려 변형이 복합적으로 발생하여 잡음도 발생한다. 하지만 사람들은 컴퓨터 비전을 쉬운 문제로 여기는 경향이 짙다. 누군가 컴퓨터 비전 전문가에게 개와 고양이를 구분하는 비전 알고리즘이 있냐고 질문했다고 하자. 마땅한 알고르즘이 없다고 대답하면, 이미 가지고 있는 프로그램을 적당히 다시 학습시켜 만들 수 있지 않느냐는 핀잔을 들을 수도 있다. 이러한 오해는 인식 문제가 사람에게 너무 쉽기 때문에 발생한다. 사람은 어떤 것이든 보기만 하면 무엇인지, 어떤 상황인지 그냥 인삭한다. (빠른 계산은 사람이 컴퓨터를 당해낼 수 없다. 현재 PC 는 초당 10억 개의 덧셈을 틀리지 않고 수행한다. 지능적인 행위는 정반대이다. 사람은 거의 100% 정확도로 순식간에 고양이와 개를 구별하지만 컴퓨터는 그겋게 하지 못한다.)

고양이와 개를 구분하는 마땅한 알고리즘은 없지만 사람의 얼굴을 인식하는 훌륭한 알고리즘은 많다. 피카사라는 프로그램을 사용하면, 디지털 카메라로 찍은 많은 사진을 사람 얼굴에 따라 자동으로 분류하여 서로 다른 폴더에 저장해준다. 피카사의 얼굴 인식 성능은 꽤 좋은 편이다. 고양이와 개를 구분하는 문제와 사람의 얼굴을 인식하는 문제 중 어느 것이 더 어려울까? 생김새의 다름 정도를 기준으로 한다면 개와 고양이의 구분 문제가 더 쉬울 수도 있따. 그런데 얼굴 인식 알고르짐의 성능이 더 높은 이유는 뭘까? 바로 얼굴 인식의 실용성에 있다. 당장 필요한 곳이 많이 깨문에 수십 년 전부터 많은 연구가 진행되어온 덕분이다. 컴퓨터 비전 연구는 특정 응용 상황을 염두에 두고, 상황에 맞는 높은 성능의 알고리즘을 확보하려 노력하는 경향이 짙다. (곤충과 동물을 인식하고 그것들의 행위를 분석하는 주제를 다루는 워크숍이 2012년 최초로 개최되었다. 개와 고양이를 구분하는 알고리즘을 확보할 수 있는 길이 열렸다.)

지금까지 컴퓨터에 비해 사림이 뛰어난 점을 이야기하였다. 하지만 특수한 상황에서는 컴퓨터 비전이 더 뛰어난 경우도 있다. 대표적인 예를 정밀 측정에서 찾아볼 수 있따. 기계로 깍은 엔진 실린더의 지름을 0.1 mm 단위의 정확도로 측정할 때 사람 눈으로는 검사가 불가능하다. 수억 년 진화한 시각 기능이지만 이런 용도에서는 무용지물이다. 하지만 정밀 카메라와 적절한 알고리즘을 사용한다면 컴퓨터 비전으로는 충분히 측정 가능하다. 이런 종류의 검사 시스템은 이미 개발되어 현장에서 사용하고 있다.

문제를 이해하기 위해 깨달아야 하는 사실이 또 있다. 컴퓨터 비전 시스템에 입력되는 영상은 숫자로 구성된 2차원 배열이다. 숫자 배열을 보고 무엇이 있는지 인식하는 일은 그리 만만한 문제가 이니다. 무엇일까?

### 2.2. 계층적 처리

컴퓨터 비전의 처리 과정은 세 단계로 나눌 수 있다.

영상 > 전처리 > 특징 추출 > 해석 > 고급 묘사

맨 앞의 전처리 (preprocessing) 단계는 주로 영상 처리(image processing)가 담당한다. 카메라로 획득한 원래 영상을 입력 받아 사용 목적에 맞게 적절하게 처리하여 보다 개선된 영상으로 만들어준다. 잡음을 제거하거나 초점이 흐린 영상을 개선하는 등의 연산이 좋은 사례이다. 이러한 영상 처리 연산은 2장에서 배운다.

특정 추출(feature extraction) 은 영상에서 에지, 선분, 원, 코너, 텍스처 등의 특징을 추출하는 단계이다. 이 책의 3-5장까지 다루는 기법을 주로 사용된다. 6 장에서는 이들 특징의 속성값을 계산하여 특정 벡터를 만드는 알고리즘을 공부한다. 특정 응용 문제를 풀어내는 데 성공하려면, 어떤 특징을 사용할지 정하는 일이 매주 중요하다. 예를 들어, 얼굴 인식의 경우 유사하르(Haar-like) 를 사람을 탐지하는데는 HOG (Histogram Of Gradient) 특징이 우수한 성능을 보인다고 알려져 있다. 예전에는 물체 인식이나 물체 추적 문제를 해결할 때 에지나 영역 특징을 주로 사용하였다면 지금은 SIFT 나 SURF 와 같은 지역 특ㅈ징을 주로 사용한다. 전처리와 특징 추출 단계까지를 보통 저급 비전이라고 부른다.

이렇게 추출한 특징을 입력 받아 그것을 분석하고 해석하는 고급 묘사(high level description) 를 출력하는 단계를 고급비전이라 부른다. 해석하는 방식은 응용 목적에 따라 크게 다르다. 예를 들어, 얼굴 인식이 필요한 응용이라면 분류를 수행해야 하며, 의료 분야의 경우에는 의시가 세밀하게 들여다봐야 할 의심스러운 곳을 출력해야 한다. 자율 주행 자동차라면 차량의 주행 방샹을 출력해야 한다. 고급 비전에서는 응용 현장에서 수집한 지식을 담은 지식베이스를 사용하기도 한다. 또 많은 샘플 영상을 수집하여 데이터 베이스를 구성한 다음 분류기를 학습시키거나 , 문맥 정보를 이용하는 후처리 단계를 두어 성능을 극대화하는 노력을 기울이기도 한다. 이 책의 7 ~ 12 장에서 이 고급 비전을 다룬다.

그림에서 제사한 틀은 전형적인 처리 과정을 보여주는 것에 불과하다. 예를 들어, 여러 장의 영상을 이어 붙어 파노라마 영상을 만드는 시스템은 보다 큰 해상도를 갖는 영상을 출력한다. 이 경우, 해석 단계는 물체 인식 과정 없이 단지 매칭과 영상 이어 붙이기만 있다고 볼 수 있다.

### 2.3 문제 해결 도구

컴퓨터 비전은 많은 도구를 동원해 문제를 해결한다. 컴퓨터 비전이 사용하는 주요 도구가 무엇인지 살펴보자.

#### 자료구조와 알고리즘

먼저 알고리즘의 중간 처리 과정에서 발생하는 데이터를 표현할 적절한 자료구조가 필요하다. 영상을 영역으로 분할한 후, 영역 간의 연결 관계와 그들 사이의 유사성 정보를 어떻게 표현할 것인가? 원래 영상의 해상도를 점점 줄여서 다중 해상도 영상을 구축하였다면, 이것을 어떻게 표현할 것인가? 컴퓨터 비전은 주로 2차원 또는 3차원 배열, 트리, 그래프 등의 자료구조를 사용한다. 처리 속도를 올리기 위해 힙, 해싱, 탐색트리를 이용하기도 한다. 영상에서 추출한 특정이나 점, 선, 면, 초평면의 기하학적 요소는 벡터와 행렬을 사용해 표현한다.

컴퓨터 비전의 처리 단계 곳곳에서 효율적인 알고리즘을 적용하는 것은 매우 중요하다. 특히 실시간 시스템과 같이 빠른 처리가 필수인 경우에는 고속 수행을 위한 알고리즘이 더욱 중요하다. 경우에 따라 탐욕 방법(greedy method), 동적 프로그래밍(dynamic programming), 한정 분기(branch and bound) 등의 많은 알고리즘 방법론이 동원된다. 수천 내지 수만 개의 특정을 보다 빨리 매칭하기 위해 kd 트리나 해싱 같은 특수한 자료구조를 동원하여 고속 매칭 알고리즘을 구현한다.

#### 수학

켬퓨터 비전은 문제를 공식화하고 그것을 푸는 알고리즘을 도출하는 데 수학을 많이 사용한다. 행열 연산을 주로 다루는 선형 대수(linear algebra), 변화량을 츶겅하거나 극점을 탐색하기 위한 미분학(calculus), 의사 결정이나 분류를 위한 확률과 통계가 주로 사용된다. 그림은 컴퓨터 비전이 취하는 문제 풀이 방식을 개념적으로 설명한 것이다.

최적화 문제로 공식화(비용 함수의 정의) > 최적해 찾기(해 공간 탐색)

먼저 주어진 문제를 최적화 문제로 공식화한다. 이 단계에서는 비용 함수(cost function) 을 정의하는 것이 중요하다. 비용 함수는 매개변수를 포함한다. 이제 비용함수를 최소화 또는 최대화하는 매개 변수 값을 찾아 그 값을 해로 취하면 된다. 이렇게 찾은 해는 경우에 따라 영역 분할 결과가 되거나 최적의 분류가 되기도 하며 인식 결과 또는 영상 파싱 결과가 되기도 한다. 최적해를 찾는 과정은 미분에 많이 의존하므로 미분에 대한 이해가 필요하다.

한편, 고등학교 수학에서는 대부분 분석적 방법으로 해결하는 문제를 다룬다. 예를 들어 이차 방정식의 근을 구하는 문제를 생각해보자. 이 방정식을 이리저리 바꿔가며 식을 유도하며 특정 근의 공식을 얻을 수 있다. 이것은 단순한 수학으로 매개변수가 하나뿐이다. 컴퓨터 비번에서 다루는 문제는 매개 변수가 수삽개인 경의도 있고, 수백 또는 수천개에 이르기도 한다. 분석적인 풀이가 불가능한 경우가 많기 때문에, 초기 해를 설정하고 어떤 과정을 반복하여 이것을 최적해에 조금씩 접근시키는 수치적 방법을 많이 사용한다. 항상 최적해에 도달하는 것은 아니고 최적해에 가까운 부분 최잭해에서 끝내고 그것에 만족해야 하는 경우도 많다.

#### 기계학습

최근들어 데이터가 복잡해지고 많아진 탓에 기계학습을 도구로 활용하는 사례가 늘고 있다. 다루는 영상의 크기도 수천 x 수천으로 커졌고, 인터넷에서 다량의 영상을 수집할 수 있게 되었다. 이 현상은 고전적인 규칙 기반의 접근 방법에는 불리하지만 현대적인 기계 학습 방법에는 유리하다. 수많은 데이터를 모두 만족하는 규칙을 만드는 것은 거의 불가능한 반면, 기계 학습은 충분히 많응 양의 데이터를 사용할수록 성능이 높아지기 때문이다. 신경망, SVM, 에이더부스트(AdaBoost), 임의 숲(Random forest) 같은 분류기를 비롯해 MRF(Markov Random Field) 도 학습을 통해 만들어진다. 이전에는 기계 학습을 고급 비전에 국학해서 활용하였는데, 최근에는 저급 비전에 활용하는 사례가 늘고 있다.

## 3. 시스템 설계

시스템을 설계하는 목적은 사람과 상황에 따라 다양할 것이다. 새로운 알고리즘을 창안하여 연구 논문을 발표하려는 대학원 학생과 교수, 최첨단 시제품을 개발하여 회사나 연구소의 명성과 이익을 높이려는 연구 개발 팀, 당장 현장에 투입할 제품을 개발하려는 회사의 개발 팀, 컴퓨터 비전 과먹을 수강하며 교수가 내준 프로그래밍 과제를 수행하는 학부나 대학원 학생마다 각자의 목표와 상황에 따라 공부할 범위와 깊이가 달라진다. 이러한 차이에도 불구하고 이들은 모두 그림의 시스템 설계 과정을 거쳐야 한다.

문제 이해 > 데이터베이스 수집 > 알고리즘 설계와 구현 > 성능 평가

시스템 개발은 그림과 같이 순차적으로 일어나지만, 순서에 따라 각 단계를 한번씩 거쳐 끝나는 것이 아니고 성능에 만족할 때까지 여러 번 피드백한다. 예를 들어, 데이터베이스를 수집한 후 그것을 면밀히 관찰하면 문제에 대한 이해가 더 깊어지고 결국 알고리즘 설계에 영향을 미친다. 평가 결과가 만족스럽지 못하면 문제의 제약 조건을 더 강하게 설정하거나 데이터베이스를 확장할 수도 있고, 알고리즘을 새로 설계하기도 한다.

### 3.1. 문제 이해

무엇보다 자신이 맡은 문제에 대한 직관적이고 철저한 이해가 선행되어야 한다. 때때로 자신이 제약 조건을 설정하고 문제 범위를 결정할 수 있는 상황을 만날 수 있다. 이런 경우에는 면밀한 검토를 통해 적절하고 합리적인 제약 조건을 정한 후, 다음 단계로 진입하는 것이 바람직하다. 얼굴 인식리를 예로 들면, 정면 얼굴을 대상으로 할지 특별한 제약 조건이 없느 영상을 대상으로 할지 결정해야 한다. 전자는 개발은 쉽지만 사용할 수 있는 응용이 제한되고 후자는 그 반대이다. 또한 전자의 경우 출입문 보안 장치로는 사용할 수 있지만 일반적인 응용에 활용할 수 없다. 이번에는 다른 예를 들어보자 앞에서 살펴본 번호판 영상은 고정된 카메라에서 얻은 영상으로, 번화판의 크기와 각도가 비교적 일정하다. 하지만 지붕에 카메라를 달고 다이며 주정차 위반을 단속하느 차량에서 찍은 번호판은 변화의 폭이 훨씬 클 것이다. 전자의 상황을 가정하고 개발한 알고리즘을 후자 상황에 그대로 적용한다면 십중팔구 무용지물이 될 것이다.

### 3.2. 데이터베이스 수집

컴퓨터 비전 시스템의 개발은 고품질의 데이터베이스를 확보한 후 진행해야 효과적이다. 이 작업의 중요성을 알지 못한 채 부실한 데이터베이스로 개발을 시작하면 나중에 문제가 생길 수 있다. 예를 들어, 자동차 번호판 인식기를 개발하기 위해 고급 승용차가 주로 다니는 도심의 길목에서 샘플 영상을 수집했다고 하자. 결과적으로 낡은 번호판을 단 차량의 영상이 적어 간단한 규칙 기반 알고리즘으로 높은 성능을 얻을 수 있다. 이렇게 개발한 시스템을 도로 곳곳에 설치하면 낡은 번호판을 달고 과속을 즐기는 난폭한 운전자를 기쁘게 해 줄 것이다. 데이터베이스의 양적으로도 커야하며 질적으로도 충분히 다양한 스타일의 영상을 담고 있어야 한다. 우수한 데이터베이스는 구축하는 데 비용이 많이 들지만 연구자나 개발자에게 큰 재산이 된다.

수집한 샢믚릉 보통 두 개의 집합으로 나누어 놓는다. 이 중 시스템을 학습시키는데 사용할 집합을 학습 집합(learning set) 또는 훈련 집합(training set)이라 부르고, 완성된 시스템의 성능을 평가하는데 사용할 집합을 테스트 집합(test set)이라 부른다. 이들을 합쳐 보통 데이터베이스라고 한다. 데이터베이스를 이용하여 시스템을 학습시키려면 샘플 영상에 적절한 표지(label)을 붙여줘야 한다. 표지의 형태는 문제에 따라 다양한데 그림에서 몇 가지 사례를 볼 수 있따.

그림은 물체 인식 성능을 겨루는 PASCAL VOC 에서 제공하는 데이터베이스로, 각각의 영상에 대해 물체의 부류와 위치를 표시해 두었다. 이러한 표지 정보를 그라운드 트루스 (ground truth) 라 부르는데, 컴퓨터 비전 시스템이 알아내야 하는 정답이라고 생각하면 된다. 그림은 차량 번호판에서 떼어낸 글자 영상으로ㅓ, 이때 표지는 글자의 부류이다. 그림은 UC 버클리에서 제공하는 영역 분할용 데이터베이스이다. 영상에 각각에 사람이 분할한 결과를 표시해 두었다. 이 경우 영역이 표지에 해당한다. 실용적인 컴퓨터 비전 시스템을 구축하는 일 뒤에는 이러한 각고의 노력이 숨겨져 있다.

다행히 굳이 직접 수집하지 않더라도 인터넷에서 구할 수 있따. 컴퓨터 비전 연구자 커뮤니티에서는 연구 개발에 활용할 수 있도록 수집한 데이터베이스를 공개한 경우가 많다. 주로 명성이 높은 연구 그룹에서 공개하며 시간이 지나면 표준 데이터베이스로 자리잡곤 한다. 이제 컴퓨터 비전 커뮤니티는 연구 논문을 평가할 때 표준 데이터베이스 사용 여부를 중요한 평가 지표로 삼는 경향이 강하다. 따라서 표준 데이터베이스를 다운받아 자신의 컴퓨터에 담아 놓은 순간 새로운 연구가 시작된다고 보아도 될 정도로 데이터베이스는 중요한 요소가 되었따.

### 3.3. 알고리즘 설계와 구현

그림에서 본 바와 같이 컴퓨터 비전을 응용할 수 있는 분야는 무척 다양하며, 시스템이 동작하는 환경과 제약 조건에 따라 변화의 폭도 크다. 주어진 문제를 정확히 이해한 후 그 문제에 적합한 알고리즘을 새로 개발하거나 기존 알고르짐 중에서 그 문제에 가장 우수한 성능을 보이는 것을 선택하는 일은 아주 중요하다.

컴퓨터 비전의 처리 절차는 그림에서 본 바와 같이 여러 단계를 거친다. 각각의 단계는 여러 세부 문제로 구성되며, 이들 문제를 푸는 많은 종류의 알고리즘이 개발되어 있다. 사람의 손동작을 인식하는 문제를 생각해보자. 손을 찾아내는 단계에서는 손 모델을 이용하여 매칭 연산을 한다. 에지나 영역을 사용해 연산하거나 SIFT 와 같은 지역 특징을 사용할 수도 있다. 영역을 사용하기로 결정햇다면, 여러 영역 분할 알고리즘 중에 어떤 것을 사용할지 결정해야 한다. 이러한 방법론적 다양성은 어려 영역 분할 알고리즘 중에 어떤 것을 사용할지 결정해야 한다. 이러한 방법론적 다양성은 무엇을 뜻할까? 바로, 자신의 문제에 가장 적합한 알고리즘을 선별하는 작업이 어려울 뿐 아이나 아주 중요하다는 점이다.

좋은 알고리즘을 찾기 위한 가장 확실하고 널리 사용하는 방버은 데이터베이스를 이용하여 실제 성능 실험을 수행하고, 그 결과에 따라 알고리즘을 선택하는 것이다. 보통 적절한 알고리즘을 찾을 때까지 다양한 알고리즘을 적용해 보는 휴리스틱한 방법을 사용한다. 이때 주어진 문제에 대한 통찰력과 공학적인 경험을 갖추고 있다면 시행착오를 줄일 수 있다. 이 책은 이러한 능력을 갖추는데 좋은 길잡이 노릇을 해 줄 것이다. 이 책은 주제별로 대표적인 알고리즘을 제시하는데, 그것들의 기본 원리를 대비시켜 좀더 깊이 이해할 수 있도록 도울 것이며 실제 응용과 관련 지어 장단점을 비교해 실용 시스템을 구축하는데 필요한 통찰력을 길러줄 것이다.

좋은 알고리즘을 선별하는데 크게 도움이 되는 길잡이가 또 있다. 요즘 두드러진 연구 방향 중하나로, 표준 데이터베이스와 표준 성능 지표를 이용하여 여러 알고르즘의 성능을 객관적으로 비교 분석하는 일이다. 컴퓨터 비번에 관련된 학술대회나 학술지에는 이러한 연구 결과를 담은 논문이 많다.

##### 프로그래밍된

컴퓨터 비전 시스템을 구현할 때에는 기본 자료구조를 살계하고 영상을 읽고 저장하는 일과 같은 기초적인 기능을 지원하는 프로그래밍 환경을 활용하는 것이 좋다. 인텔에서 개발한 OpenCV 는 이러한 기본 기능은 물론 컴퓨터 비전 분야 전체를 망라하는 알고리즘을 구현하여 라이브러리 형태로 제공한다. 특히, C/C++ 언어에 익숙한 사람에게 유용하다. 오픈 소스 소프트웨어로, 무료로 제공된다. OpenCV 는 컴퓨터 비전 기술의 발전과 더불어 빠르게 진화하고 있으므로 최산 버전을 사용하기 바란다.

Mathlab 은 C/C++ 와 같은 범용 프로그래밍 언어에 익숙하지 않은 사람이 쉽게 접근할수 있는 대안이다.. Mathlab 은 원래 수학 패키지로 개발되었지만 현재 IPL(Image Processing Toolbox) 라는 이름으로 컴퓨터 비전용 함수도 다수 제공하고 있다. Mathlab 은 적은 수의 명령어로 알고리즘을 구현할 수 있으므려, 어떤 알고리즘의 동작이나 성능을 단시간 내에 파악하는 목적으로 적합하다. 하지만 수행 속도가 느리기 때문에 현장에 설치할 시스템을 개발할 때는 C/C++와 같은 범용 프로그래밍 언어로 구현하는 것이 좋다.

또한 이들 외에 다양한 소프트웨어가 인터넷에 공개되어 있다.

#### 3.4. 성능 평가

엄밀한 성능 평가는 컴퓨터 비전 시스템을 개선하는 동력이 된다. 성능을 측정하는 방법은 문제 및 요구 사항에 따라 아주 다양하다. 여기에서는 보편적으로 사용하는 몇 가지 지표만 설명한다. 더욱 폭넓은 성능 평가 방법을 공부ㅏ고 싶다면 Christensen2002 를 참고하기 바란다.

자동차를 찍은 영상이 들어왔을 때 그것을 세단, RV, 버스, 트럭, 트레일러의 다섯 부류로 분류하는 시스템을 생각해보자. 이 시스템의 인식 성능을 측정하려면 맞는 샘플(버스를 버스로 분류하는 경우), 틀리는 샘풀(세단을 트럭으로 분류하는 경우), 기각하는 샘플의 갯수를 세어 식의 정인식률(correctt recognition rate), 기각률(rejection rate), 오류율(error rate)를 계산하면 된다. 이 식에서 N 은 테스트 집합의 크기, 즉 테스트 집합이 가진 샘플 영상의 개수이다. 기각이란 시스템이 결과에 자신이 없어 분류를 포기한 경우를 말한다. 대부분의 분류기는 매개변수를 설정하여 어느 정도 기각할지 조정할 수 있따.

정인식률= C/N

기각률 = r/N

오류율 = e/N

c = 맞는 샘플 수,

r = 기각한 샘플 수,

e = 틀린 샘플수

N = c + r + e

오류의 경향을 더욱 세밀하게 분석할 때는 혼동 행렬(confusion matrix) 을 사용한다. 표는 부류가 두 개인 이전 분류의 혼동 행렬을 보여주는데, 앞의 자동차 인식 예처럼 다섯 부류인 경우 단순히 열과 행을 다섯 개로 확장하면 된다. n ij 는 부류 w i 에 속하는 샘플을 w j 로 분류한 것의 계수이다. 예를 들어, n 11 은 w 1을 w 1 으로 옳게 분류한 샘플의 개수이고, n 12 는 w1 을 w2 로 틀리게 분류한 샘플의 개수이다. 혼동 행렬을 살펴보면 분규기가 어떤 부류를 다른 부류로 혼동하는지 경향을 일목요연하게 파악할 수 있다.

| 분류결과 | W 1 | W 2 |
| ------ | --- | --- |
| W1     | N11 | N12 |
| W2     | N21 | N22 |

폐 영상을 보고 폐암 환자를 찾거나, 반도체 칩 영상을 보고 불량품을 찾거나, 사진 영상에서 얼굴을 찾는 일과 같이 어떤 대상을 검출하는(detectoin) 문제는 검출하거나 하는 긍정 부류 w1 과 그렇지 않은 부정 부류 w2 로 구별하는 이전 분류 문제이다. 이때 성능 지표를 사용할 수 있지만 많은 경우 그것만으로 부족하다. 왜냐하면 긍정과 부정이 심한 불균형을 이룬 경우가 많기 때문이다. 예를 들어 칩 검사에서 불량률이 0.1% 라면, 무턱대고 모두 우량품이라고 말해도 정인식률이 99.9% 가 된다.

혼동행렬에서 보듯 이진 분류의 결과는 네 가지로 나눌 수 있따. w1 을 w1 으로 옳게 분류한 샘플링은 참 긍정(TP - TRUE POSTIVE), w2 를 W2 로 옳게 분류한 샘플을 참 부정 (True negative), W1 을 W2 로 틀리게 분류한 샘플은 거짓 부정 (FN - FALSE NEGATIVE), W2 를 W1 으로 틀리게 분류한 샘플은 거짓 부저 (FN - FALSE NEGATIVE), 이라고 한다. FP는 거짓 검풀 FD - FALSE DETECTION 또는 거짓 정보 (FA = FALSE ALARAM) 이라고 한다. 불량 검풀의 경우, 찾는 것이 불량이므로 불량이 긍정에 해당하고 우량은 부정이다. 이 경우에는 거짓 긍정 대신 거짓 정보라는 용어가 적절할 수 있따/

표의 혼동 행렬에서 n11, n12, n21, n22 는 각가 TP, FN, FP, TN 으 개수이다. 검출 시스템의 성능은 보통 아래의 식으로 정의되는 거짓 긍정률(FPT = FALSE POSTIVE RATE) 과 거짓 부정률 (FNR = FALSE NEGATIVE RATE)로 측정한다. 식으로 정의되는 정확률(precision)과 재현율(recall) 로 측정하기도 한다. 정확률은 찾은 것(TP 와 FP) 중에 맞게 찾은 것(TP)의 비율이고 재현율은 찾아야 하는 것(TP 와 FN) 중에 맞게 찾은 것(TP) 의 비율이다.

거짓 긍정률 (FPT = FP / (FP + TN))
거짓 부정률 (FNR = FN / (TP + FN))
참 긍정률 (TPF = TP / (TP + FN))
참 부정률 (TNR = TN / (FP + TN))

정확률 = TP / (TP + FP)
재현률 = TP / (TP + FN)

때로은 정확률과 재현율을 결합하여 하나의 값으로 표현하는 지표가 필요하다. 식의 F 측정(F-measure) 은 이런 경우에 사용되는 지표이다. 이때 베타는 정확률과 재현율 중 어느 것에 비중을 둘지 결정한다. 예를 들어 베타 =- 2로 하면 F2 측정이 되는데, 정확률보다 재현율에 더 큰 미중을 두는 셈이다. 보통 베타=1로 둔 정확률과 재현율을 같은 비중으로 보는 F1 측정이 많이 사용된다.

F = (1 + 베타^2) * (정확률 * 재현율) / ( 베타^2 * (정확률 + 재현율) )

F = 2 * 정확률 * 재현율 / (정확율 + 재현율)

그림과 같은 얼굴 검출의 경우는 특별히 신경을 써야 할 점이 있다. 얼굴이 긍정에 해당하고 얼굴 아닌 곳이 부정인데, 긍정은 몇 개인지 명확하지만 부정은 불문명하다. 따라서 n 22 를 셀 수 없으므로 거짓 긍정률을 계산할 수 없다. 하지만 정확률과 재현률은 n 22 를 사용하지 않으므로 계싼할 수 있다.

이절을 마치기 전에 성능과 관련하여 갱각해야 할 것이 또 있다. 그것은 강건이라는 단어로 압축할 수 있는데, 시스템이 작동하는 외부 환경이 변할 때 성능을 얼마나 잘 유지하는지를 나타낸다. 조명의 변화 또는 대상물을 찍는 거리나 각도가 변함에도 불구하고 겅능이 그대로 유지되거나 적은 양만 저하되는 경우를 강건하다고 말한다. 사람의 시각은 매우 강간하다. 컴퓨터 비전 시스템도 쓸모가 있으려면 강건해야 한다.

강건한 시스템을 구축한다는 목적을 염두에 두고 지금까지 기술한 내용을 설계 철학으러 정리해보자. 먼저 주어진 문제를 정확하게 이해하고 시스템이 동작할 환경의 제약 조건을 세밀하게 설정한다. 이 제약 조건 아레에서 각종 변화에 강건한 시스템을 설계하고 구현한다. 이 과정에서 충분히 다양하고 많은 양의 데이터베이스를 확보해야 하며, 엄밀한 성능 평가 지표를 갖추어야 한다. 여러 단ㄱ의 처리 과정 각각에 대하 최적의 알고리즘을 선정해야 한다.

## 4. 인접 학문

컴퓨터 비전과 인접한 학문이 여럿 있는데, 이들은 방법론과 응용 측면에서 적지 않게 겹친다. 대표적으로 영상 처리, 패턴 인식, 그리고 컴퓨터 그래픽스를 들 수 있다.

영상처리(image processing)는 영상을 입력으로 받아 처리하여 새로운 영상을 출력한다. 물론 새로운 영상은 주어진 목적을 달성하는데 더 적합한 형태이다. 예를 들어 스무딩(smoothing) 연산을 적용하여 잡음이 줄어든 영상을 만든다거나, 렌즈를 거치면서 왜곡된 영상을 다시 펴서 정상적인 영상으로 만드는 작업이 영상 처리에 속한다. 따라서 영상 처리는 컴퓨터 비전의 전처리 과정으로 주로 사용된다.

컴퓨터 비전은 영상을 입력 받아, 분석 및 해석하여 고급 묘사를 출력한다. 예를 들어 왼쪽 영상이 입력되면 '멀리뛰게 하는 여자 선수'라는 묘사를 출력해야 한다. 물론 응용 분아에 따라 요구되는 묘사의 종류는 다양하다. 영상 감시의 경우 이상 징후 여부, 로봇 비전의 경우 장애물의 3차원 위치가 고급 묘사에 해당한다.

컴퓨터 그래픽스는 컴퓨터 비전과 반대 과정으로 볼 수 있다. 즉, 입력된 고급 묘사를 바탕으로 영상을 생성하거나 합성한다. 물체의 이동 방향과 속도가 주어지면 물체의 이동에 따라 여러 장의 영상을 합성한 후, 연속으로 보여줌으로써 애니메이션을 만들기도 한다. 물론 '멀리뛰기 하는 여자 선수' 정도의 거친 묘사가 주었을 때 자동으로 영상을 생성할 수 있는 지증적인 컴퓨터는 없다. 아직은 장면에 나타나는 모든 물체에 대해 물체를 구성하는 평면 및 연결 관계를 지정ㅎ애ㅑ 하며, 물체 표면의 반사 특성, 평원의 위치, 조도까지도 하나하나 설정해 주어야 한다.

패턴 인시과 컴퓨터 비전의 관계를 설정하는 일은 조금 까다롭다. 패턴 인식은 얼굴아니 문자처럼 영상으로 표현되는 패턴도 다루지만, 주식 시황, 음성 신호, 고객 소비 성향, 날씨 변동과 같은 온갖 종류의 데이터를 패턴으로 간주하고 분석과 분류 작업을 수행한다. 즉 특징 추출기가 입력 대펀에서 특징을 추출하여 특정 벡터로 표현하면, 신경망이나 SVM 과 같은 분류기가 특징 벡터를 분휴하여 부류를 출력한다. 따라서 패펀 인식 연구의 핵심은 특징 추출기와 분류기의 성능을 높이려는 노력이고, 패턴 인식 교과서는 두 주제를 깊이 있게 다른다.

지금까지 관련 분야가 서로 어떻게 다른지 설명했는데, 주의할 점이 있다. 세성아느 모호한 영역을 품고 있다. 학문도 마찬가지이다. 예를 들어, 영상처리 분야의 교과서인 Gonzalez 2010 을 설펴보면 앞서 설명한 영상처리 범위를 넘어 특정 추출, 영상 분할, 물체 인식까지 다룬다.

더불이 이들 학문이 서로 협력하는 추세가 점점 강해지고 있따. 예를 들어, 컴퓨터 그래픽스로 만든 영상과 실사로 찍은 영상을 하나로 합쳐 영화를 만다는 자업은 이젠 예삿일이다. 두 가지 영상을 자연스럽게 결합시키는 것이 핵심인데, 모든 일은 사람이 수작업으로 하기에는 시간이 너무 많이 걸린다. 때문에 컴퓨터 비전 기술을 사용하여 반자동으로 처리해 시간과 비용을 절감하면, 컴퓨터 그래픽스와 컴퓨터 비전이 밀접하게 협력하는 대표적인 사례이다. 셰계저인 컴퓨터 그래픽스 학술대회에서 발표되는 눈문을 살펴보면 이러한 추세를 확인할 수 있다.

용어에도 컴퓨터 비전과 대비시켜 사용하거나 때로는 같은 의미로 사용하는 여러 용어가 있다. 기계 시각(machine vision), 로봇시각(robot vision)이 그것이다. 대체적으로 말하자면, 컴퓨터 비전은 사람 시각을 목표로 하는 과학적인 접근과 시스템을 목표로 하는 공학적안 접근을 모두 포함하는 일반적인 용어로 볼 수 있다. 반면 기계시각과 로못시각은 공학적인 접근을 강조한 측면이 있고, 그 중 로봇 시각은 로봇의 인지 기능을 높이는 분야에 국한하여 사용된다. 이러한 용어에서도 경계가 불분명한 경향이 있다. 사람에 따라 또는 상황에 따라 엄격히 구분하기도 하고 같은 의미로 사용하기도 한다는 점을 기억해 두자.

## 5. 학습을 위한 지원

- Computer Vision: Algorithms and Application - Richard Szeliski
- Image Processing, Analysis and Machine Vision - Milan Sonka, Vaclav Hlavac, and Roger Boyle
- Computer Vision - Linda G. Shapiro and George C. Stockman

영상처리 - Gonzlez 2010
패턴인식 - Theodoridis 2009, Bishop 2006, 오일석 2008

### 학술지 및 학술대회

IEEE Transactions on Pattern Analysis and Machine Intelligence

International Journal of Computer Vision

Image and Vision Computing

Computer vision and Image Understanding

Foundations and Trends in Computer Graphics and Vision

Pattern Recognition

IEEE Transactions on Image Processing

ACM Transactions on Graphics


http://homepages.inf.ed.ac.uk/rbf/CVonline/

http://www.visionbib.com/index.php

http://www.computervisiononline.com/

연습문제

1. 컴퓨터 비전의 응용에 소개하는 Lowe 교수가 운영하는 사이트에 접속한다. 사이트에 제시된 응용 분야 중에 세 가지를 고르고, 각각에 대해 회사 두 곳을 선정하여 그들 제품을 조사하시요.

http://www.cs.ubc.ca/~lowe/vision.html

2. 다음 세 가지 앱을 자신의 스마트폰에 설치하시오.

- googles
- leafsnap
- photosynth

(a) 직접 영상을 획득한 수 처리한 결과 화면을 캡쳐하여 제시하시오.
(b) 여러 번 시도한 후, 처리 결과를 분석하여 성능에 대한 자신의 견해를 제시하시오.

3. 물체 인식과 영상 분할에 쓰이는 데이터 베이스를 예시하고 있다. 이들 데이터베이스 각각에서 10개의 영상을 뽑아 제시하시요.

4. 거싲긍정률과 거짓 부정률을 걔산하시오.

- 그림의 가운데 영상에서는 두번째 줄에 잇는 파란색 스카프를 두란 아이의 얼굴을 찾지 못했다. 이 아이의 얼굴을 찾았다고 가정했을 때 참 긍정, 거짓 긍정, 거짓 부정을 구하시요. 이때 재현율, 정확률, 그리고 F1 측정은 얼마인가?

5. 1.5 절에서 소개한 학술지와 학술대회 목록 각각에서 두 종을 고르고 가장 최산 호에 실린 논문 세개씩을 선정하여 제목, 저자, 초록을 제시하시오, 또한 논문 각각에 대해 이책의 몇 장에 해당하는지를 기술하시요.

6. Marr 상을 조사하여, 최근 3년의 수상자와 그에게 상을 안겨준 논문의 제목과 초록을 제시하시오.

## 2. 영상처리

### 2.1. 디지털 영상이란?

#### 2.1.1. 디지털 영상의 태동

흥미롭게도 디지털 영상의 응용은 신문 산업에서 태동했다. 1920년에 Bartlane 이라는 케이블 영상 전송 시스템이 등장하였다. 이전에는 영상을 배편으로 전달했는데, 유럽 대륙에서 출발해 대서양을 거쳐 미국에 도달하기까지 몇 주가 걸렸다. 그러다 보니 영상을 인쇄할 때쯤이면, 해당 뉴스는 사람들 관심 밖으로 밀려서 사치가 크게 떨어지곤 했다. 그런 참에 런던과 뉴욕을 해저 케이블로 연결한 영상 전송 시스템인 Bartlane 이 등장한 것이다. 두 시간 정도면 영상을 전달할 수 있게 되자, 미디어 산업에 혁명이 얼어났다. [그림 2-2]는 그 당시 전송된 사진이다. 왼쪽은 초기 영상으로 5 단계의 명암을 가졌는데 나중에 오른쪽처럼 15 단계로 늘어 화질이 크게 개선되었다. 보다 자세한 내용는 [McFarlane72]를 참고하기 바란다.

그림 2-2. Bartlane 시스템이 전송한 디지털 영상

대략 한 세기가 지난 지금, 초등학생 주머니에도 모바일 폰이 들어 있다. 고해상도 사진을 찍어 바로 전송할 수 있을 뿐만 아니라 고품질 동영상을 실시간으로 방송할 수도 있게 되었다. 이런 상황에 발맞추어 '컴퓨터 비전과 인터넷의 교차점'에 초점을 맞춘 인터넷 비전(internet vision)이라는 새로운 연구 분야가 태동할 정도로 영상 처리와 컴퓨터 비전은 주목을 받고 있다. 인터넷에서는 무궁무진하게 많은 영상이 공유되고 있으며 그 양도 빠르게 증하하는 추세이다. (영상 호스팅 서비스인 플리커는 2011년 8월에 60억 장의 영상이 업로드 되어 있다고 발표하였다.)

#### 2.1.2. 획득과 표현

영상을 획득하는 장비인 카메라는 사람의 눈과 비슷한 구조를 가진다. [그림 2-3]은 사람의 눈과 카메라의 구조를 비교한 것이다. 수정체는 카메라 의 렌즈 역할을 하며, 망막은 GCD 센서(필름)에 해당한다. 망막에는 밝기에 반응하는 간상체(rod)와 색에 반응하는 추상체(cone)가 분포되어 있다.

그림 2-3. 사람의 눈과 카메라의 구조

[그림 2-4 (a)]는 핀홀(pinhole) 카메라 모델로, 복잡한 카메라의 작동 원리를 단순하게 표현한 수학적 모델이다. 빛이 상자의 왼쪽에 있는 아주 작은 구명을 통해 내부로 들어가면 오른쪽 영상 평면(image plane) (GCD 센서 또는 필름)에 맺힌다. 영상 평면에 붙어 있는 GCD 센서는 영상을 가로 방향으로 N, 세로 방향으로 M 개의 점으로 샘플링하고, 화소(pixel)라고 부르는 각 점의 밝기를 L 단계로 양자화(quantization) 한다. [그림 2-4 (b)] 는 샘플링과 양자화를 수행하는 과정을 보여준다. [그림 2-4 (c)] 는 이런 과정으로 획득한 M 과 N 이 12이고, L 이 10 인 (0-9) 디지털 영상을 보여준다.

[그림 2-4] 디지털 영상 획득

[그림 2-5]는 이책에서 사용할 2차원 영상 좌표계이다. 중학교에서 배운 좌표계와 달리, 원점이 왼쪽 위에 위치한다.

[그림 2-5] 디지털 영상의 좌표계

여기에서는 2차원 좌표를 벡터 x = (j, i) 또는 x = (y, x) 로 표기한다. f 축은 수직 방향, i 축은 수평 방향을 나타내며 둘 다 정수 좌표를 가진다. 영상은 두 개의 매개변수를 가진 일종의 함수이며 f(x) 또는 f(j, i) 로 표기할 것이다. 또한 j 축 방향의 화소의 개수는 M, i 축 방향은 N 이라 표기한다. 따라서 j 와 i 의 범위는 [0, M - 1]과 [0, N - 1] 이다. (j, i) 로 지정되는 한 점을 화소(pixel)라 하고, 영상의 크기를 나타내는 M x N 을 해상도(resolution) 이라고 부른다.

더불어 영상 f 가 가질 수 있는 명암값의 범위는 [0, L - 1] 이며 보통 화소 하나에 1 바이트를 매정하여 L 이 256 이 된다. L = 2 인 경우에는 0(흑)과 1(백)의 두 가지 값만 가능하며, 이런 영상을 이진영상(binary image)이라 부른다. 컬러 영상은 한 화소가 R, G, B 세 개의 값을 갖느다. 이들 세 개의 채널을 f()
